---
title: "81-compute-suicidal"
output: html_document
---

#81-compute-suicidal

This notebook is used with SLURM for the purpose of computing all data regarding suicidal ideation; this includes bootstrap model performance on validation sets, permutation importance tables, and MDI tables among others.  See `81-interpret-suicidal` for more information on data selection, predictors, and outcome variables.

## Usage
To use this file, you'll need to actually run it from slurm_experiments.R.  This will drop in some of the variables which are not present here, including `n_boot`, `seed`, `task_ID`, and `results_directory`.

```{r load libraries, include=FALSE}
# Use pacman, which forces an install if the library isn't present on the running machine
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, h2o, furrr)
```

```{r source files}
rsource_dir = 'r_project_source'
source_files_list <- list.files(rsource_dir)
map(str_c(rsource_dir, '/', source_files_list), source)
```

```{r initializations, include=FALSE}
h2o.init() 
h2o.no_progress()
future::plan(multiprocess)
```

```{r seeds for reproducibility}
# define seed (defined in slurm_experiments.R)
seed = 9384
h2o_seed = -1
set.seed(seed)
```

# Dataset generation

The predictors we will be using will be several variable lists loaded from `10-import-data.Rmd` file. These initial set of predictors will be based of the list of variables that describe anxiety, depression, optimism, some demographics, and biological despair.

```{r load raw data and formulate dataset, warning=TRUE, message=TRUE}
## set outcome variable of interest
outcome = 'h5mn8'
filebase = '/scratch/p_gaydosh_lab'

#create data in specified form
dataset_list <- generate_datasets(outcome, legit_skip=FALSE, filebase=filebase, seed_val=seed)

#parse out dataset components
full_dataset <- dataset_list$full_dataset
ds_raw <- dataset_list$ds_raw_outcome
ds <- dataset_list$ds_final

#ml splits of the data
training_df <- dataset_list$training_df
validation_df <- dataset_list$validation_df
testing_df <- dataset_list$testing_df

```


# Robust feature evaluation {.tabset .tabset-fade .tabset-pills}

## RF model

```{r feature selection rf, include=FALSE}

# Spans of hyper parameters for random forest
rf_params <- list(max_depth = 50,
                  ntrees = 150,
                  mtries = c(-1, 20, by=3),
                  min_rows = c(2, 15, by=3),
                  stopping_rounds = 3,
                  #balance_classes = c(TRUE, FALSE),
                  stopping_metric = 'AUCPR',
                  categorical_encoding = 'one_hot_explicit')

# define number of bootstraps (defined in slurm_experiments.R)
#n_boot = 2

boots_rf <- model_feature_selection("RF",training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      outcome = outcome, n = n_boot, seed=seed, h2o_seed = h2o_seed)

```

```{r get rf bootstrap model seeds and save}
bs_rf_seeds <- get_model_seeds(boots_rf$models)

save_to_csv(bs_rf_seeds, results_directory, task_ID)

```


```{r save/load rf bootstrap model performance, include=FALSE}

bs_rf_perf <- get_metric_set_from_perfs(boots_rf$perfs) %>%
  dplyr::select(accuracy, mpce, sens, spec, ppv, npv, roc_auc, pr_auc,
                tns, tps, fns, fps, no_n, no_p,  err_rate, bal_accuracy, everything())

save_to_csv(bs_rf_perf, results_directory, task_ID)

```

### Feature importances: Random Forest
#### Mean decrease in impurity (MDI)

```{r save/load rf bootstrap model mdi, include=FALSE}

bs_rf_mdi <- boots_rf$mdi %>% 
  purrr::reduce(rbind)

save_to_csv(bs_rf_mdi, results_directory, task_ID)

```


#### Permutation importance
Calculate and save permutation importance

```{r compute/save or load bs rf model permutation, include=FALSE}

bs_rf_perm_plt <- boots_rf$models %>%
  get_aggregated_permute_imp(training_df, outcome=outcome)

save_to_csv(bs_rf_perm_plt, results_directory, task_ID)

```


## LASSO model
In this step, we model the relation between the outcomes and the predictors using a linear regression with L2 regularization.  This drives the importance of unimportant and redudant features towards zero.

```{r feature selection lasso, message=FALSE, warning=FALSE}
# Function parameters
lasso_params <- list(alpha = c(1))

boots_lasso <- model_feature_selection( "Lasso",
                                          training_frame = training_df,
                                          validation_frame = validation_df,
                                          hyper_params = lasso_params,
                                          outcome = outcome, 
                                          n = n_boot,
                                          seed=seed,
                                          h2o_seed=h2o_seed)
```

```{r get lasso bootstrap model seeds and save}
bs_lasso_seeds <- get_model_seeds(boots_lasso$models)

save_to_csv(bs_lasso_seeds, results_directory, task_ID)
```

```{r save/load lasso bootstrap model performance, include=FALSE}

bs_lasso_perf <- get_metric_set_from_perfs(boots_lasso$perfs) %>%
  dplyr::select(accuracy, mpce, sens, spec, ppv, npv, roc_auc, pr_auc,
                tns, tps, fns, fps, no_n, no_p,  err_rate, bal_accuracy, everything())

save_to_csv(bs_lasso_perf, results_directory, task_ID)

```

### Feature importances: LASSO
#### Coefficient-based variable importance

```{r save/load lasso bootstrap model mdi, include=FALSE}

bs_lasso_mdi <- boots_lasso$mdi %>% 
  purrr::reduce(rbind)

save_to_csv(bs_lasso_mdi, results_directory, task_ID)

```

#### Permutation importance

```{r compute/save or load bs lasso model permutation, include=FALSE}

bs_lasso_perm_plt <- boots_lasso$models %>%
  get_aggregated_permute_imp(training_df, outcome=outcome)

save_to_csv(bs_lasso_perm_plt, results_directory, task_ID)
  
```

# Generation of final model {.tabset .tabset-fade .tabset-pills}

## Add folds to training set
```{r join with kfold assignments}
if(!is.null(kfold_file)){
  
  #read kfold assignments
  kfold_assigns = read_csv(kfold_file, col_types = cols(aid=col_character(), fold_assign=col_factor()))
  
  #join with training_df
  start_len = nrow(training_df)
  training_df <- full_join(training_df, kfold_assigns, by='aid')
  
  #make sure that all of these things have the same length
  if((nrow(training_df)!= start_len) | nrow(training_df) != nrow(kfold_assigns)){
    stop('training df, kfold_assigns, and the joined df do not appear to have the same number of rows!')
  }
}
```

## RF model
In this step, we build the final model for the random forest.  We use slightly more values in order to come up with the best model, keeping in mind the number of combinations that are required to run to evaluate the grid.
```{r final model evaluation rf}

# # Spans of hyper parameters for random forest
rf_params <- list(max_depth = 50,
                  ntrees = 150,
                  mtries = seq(-1, 20, by=5),
                  min_rows = seq(2, 20, by=2),
                  stopping_rounds = 3,
                  #balance_classes = c(TRUE, FALSE),
                  stopping_metric = 'AUCPR',
                  categorical_encoding = 'one_hot_explicit')

# Function parameters
final_model_rf <- rf_model(outcome,
                           training_frame = training_df,
                           validation_frame = validation_df,
                           #nfolds = 5,
                           fold_column = 'fold_assign',
                           hyper_params = rf_params, model_seed=h2o_seed)

```

```{r get rf final model seeds and save}
final_rf_seeds <- get_model_seeds(list(final_model_rf[[1]]))

save_to_csv(final_rf_seeds, results_directory, task_ID)
```

```{r save/load final rf model}

#get performance and model
final_rf_perf <-final_model_rf[[2]]
final_rf_model <- final_model_rf[[1]]

#save performance to file
save_to_csv(final_rf_perf, results_directory, task_ID)

#save model to file
save_h2o_model(final_rf_model, results_directory, task_ID)

```

### Features: permutation importance

```{r compute/save or load final rf model permutation}

final_rf_perm_plt <- list(final_model_rf[[1]]) %>%
  get_aggregated_permute_imp(dplyr::select(training_df, -fold_assign),
                             outcome=outcome)

save_to_csv(final_rf_perm_plt, results_directory, task_ID)

```


## LASSO model
Now, we create the final model for LASSO.  There is no substantial difference between this method and the bootstrap methods, other than the data upon which the model is being built.
```{r final model evaluation lasso, message=FALSE, warning=FALSE}
# Function parameters
lasso_params <- list(alpha = c(1))

final_model_lasso <- lasso_model(training_frame = training_df,
                                 validation_frame = validation_df,
                                 outcome = outcome,
                                 #nfolds = 5,
                                 fold_column = 'fold_assign',
                                 hyper_params = lasso_params,
                                 model_seed = h2o_seed)
```

```{r get lasso final model seeds and save}
final_lasso_seeds <- get_model_seeds(list(final_model_lasso[[1]]))

save_to_csv(final_lasso_seeds, results_directory, task_ID)
```

```{r save/load final lasso model}
  
#get performance and model
final_lasso_perf <-final_model_lasso[[2]]
final_lasso_model <- final_model_lasso[[1]]

#save performance
save_to_csv(final_lasso_perf, results_directory, task_ID)

#save model
save_h2o_model(final_lasso_model, results_directory, task_ID)

```

### Features: permutation importance

```{r compute/save or load final lasso model permutation}

final_lasso_perm_plt <- list(final_model_lasso[[1]]) %>%
  get_aggregated_permute_imp(dplyr::select(training_df, -fold_assign),
                             outcome=outcome)

save_to_csv(final_lasso_perm_plt, results_directory, task_ID)

```

