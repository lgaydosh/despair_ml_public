---
title: "53-modeling-imbalances"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: lumen
    code_folding: hide
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Investigating the role of imbalanced classes on random forest performance

In this notebook, we explore how having imbalanced classes will affect the performance of the random forest.  There seem to be two trains of thought in the literature regarding class imbalances and machine learning (ML) algorithms:

1. One *should not* correct class imbalances during training; this is because your data should reflect the underlying distribution of your population.  The ML model should thusly generate predictions according to this expected distribution of classes.
2.  One *should* correct the class imbalance during training; this is because your ML model should be distribution agnostic, and should be able to accurately discriminate between two classes, regardless of the underlying distribution of classes.  Thus, you should provide your model every opportunity to learn more about the class which it would see less of during training.

This appears to be a point of major contention in the ML community, although it seems as if most practitioners lean towards maintaining the class imbalance during training.  However, with random forests (RF), there seems to be strong arguments towards balancing the classes, although the RF would have the same issues a regular ML model would have - learning less about the less represented class.  Perhaps this is simply more of an issue for RFs, so we investigate the behavior here for our purposes.

### Libraries

```{r}
pacman::p_load(purrr, tidyverse, h2o)
```

### Constants

```{r}
seed = 2435
set.seed(seed)
```

### Helper functions

```{r}
get_inner <- function(in_list, var_name='aid'){
# This function returns values that are common to all lists

  all_aid_list <- in_list %>%
    map(select, var_name) %>%
    purrr::reduce(intersect) %>%
    pull(var_name)
  
  return(all_aid_list)
}
```


## Generate dataset of interest

```{r load all wave files}
wave_data <- load_waves(1:5)
```

```{r join all waves}
full_dataset <- get_working_dataset_full(wave_list, join_type = 'full')
```

```{r}
# set outcome variable of interest
outcome = 'h5mn8'

# get the aids that you want
inner_aids <- get_inner(list(wave_list[[1]], wave_list[[3]], wave_list[[4]], wave_list[[5]]))

# use the features and ids that you want to select out what you want
working_ds <- full_dataset %>%
                filter(aid %in% inner_aids) %>%
                select(c(aid, predictor_list, outcome))

```

## Generate training and testing set of interest

Note: for reproducibility, it looks like we set a seed using `set.seed()`.  TODO: Determine exactly which random number generators this sets.

```{r}
#create stratified split
splits <- working_ds %>%
  rsample::initial_split(strat=outcome, prop=0.8)

train <- rsample::training(splits)
test <- rsample::testing(splits)
```

Let's make sure that the class distributions are correct.  Note that ACCRE's version of tidyr is 0.8.*, so I can't use pivot_wider!

```{r}
orig_dist <- working_ds %>%
  group_by_at(outcome) %>% #allows you to group by a variable name 
  summarise(cnts = n()) %>%
  mutate(ratio = cnts/sum(cnts), name='orig') %>%
  select(-cnts)

train_dist <- train %>%
  group_by_at(outcome) %>% 
  summarise(cnts = n()) %>%
  mutate(ratio = cnts/sum(cnts), name='train') %>%
  select(-cnts)

test_dist <- test %>%
  group_by_at(outcome) %>% 
  summarise(cnts = n()) %>%
  mutate(ratio = cnts/sum(cnts), name='test') %>%
  select(-cnts)

dists <- bind_rows(orig_dist, train_dist, test_dist) %>%
  spread(h5mn8, ratio)
```

The distribution of these data seem to be rather the same which is desired. Let's also just quickly make sure that the sets are disjoint.

```{r}
# intersect aids and make sure the size of the intersection is 0
ml_dataset_isect <- get_inner(list(train, test))
length(ml_dataset_isect)
```
Good!  There's no intersection of aids in the list.  Now, let's do some modeling.

## Let's generate some models

Note: this uses the following helper function below (`boot_model_rf`), which is very similar to the one found in 40-feature-importance.  A few changes have been made to embue the desired stratification, as well as some tailoring to this specific application.

I'm going to look at three things in particular:

0. `mean_per_class_error` in `stopping metric`: this evaluates the rf based on the mean per class error.  This may solve our problem
1. `balance_classes`: this balances the training set from the outset, but bootstrap sampling within the trees occurs however
2. `sample_rate_per_class`: this forces the bootstrap samples to also be stratified

I'm going to replicate each endeavor a couple times just to get an average.

```{r}
h2o.init()
```

I'm going to also formulate the hyperparameters as a function of the number of features in the dataset.

```{r}
# model hyperparameters
no_rows <- nrow(working_ds)
no_cols <- ncol(working_ds)
tree_span <- c(0.1, 1, 10, 50)
max_depths_span <- c(0.1, 0.5, 1, 2)
mtries_span <- c(0.1, 0.14, 0.2, 0.5)
repeats = 10
```

Here, I'm going to just investigate the default behavior

```{r}
default_res <- working_ds %>%
  select(-aid) %>%
  boot_model_rf(outcome, as.integer(max_depths_span*no_cols), 
                         as.integer(tree_span*no_cols),
                         as.integer(mtries_span*no_cols), seed=seed)
```

Now, let's look at using the `mean_per_class_error` as a metric:

```{r}


```


### Helpers, but will be unnecessary after revision (4/6/2020)
```{r convenience functions}

##################
# Helper function to create bootstrapped models for RF models
boot_model_rf <- function(train, outcome, max_depths_tune, ntrees_tune,
                          mtries_tune=-1, nfolds=5, stopping_metric='AUTO', seed=2435){  
  # train : bootstrapped training frame sent in to be created a model with
  
  # use a stratified split to split into validation vs training
  straps_splits <- rsample::initial_split(as.data.frame(train), strat=outcome, prop=0.9) 
  
  # pull out training and validation frames
  training_frame <- as.h2o(rsample::training(straps_splits))
  validation_frame <- as.h2o(rsample::testing(straps_splits))
  
  # Create all combinations of tuning parameters
  params <- expand.grid(max_depth = max_depths_tune, ntrees = ntrees_tune, mtries=mtries_tune)
  models <- c()
  performances <- c()
  
  #initialize function parameters for h2o.randomForest
  outputs <- outcome
  inputs <- setdiff(names(train), outcome)
  
  #Training models on all combinations of parameters
  for(i in 1:nrow(params)){
    rf_i <- h2o.randomForest(
      training_frame = training_frame,
      x= inputs,
      y= outputs, 
      #model_id = "rf_covType_v1",
      ntrees = params[i,][[2]],
      max_depth = params[i,][[1]],
      mtries = params[i,][[3]],
      stopping_metric = stopping_metric,
      stopping_rounds = 2,
      nfolds = nfolds,
      fold_assignment = "Stratified",
      #score_each_iteration = T,
      seed = seed
    )
    
    #Save the model and performance on validation set to assess full model performance
    models <- models %>%
      append(rf_i)
    
    performances <- performances %>%
      append(h2o.performance(rf_i, validation_frame))
    
  }
  
  return(list(mods = models,
              perfs = performances,
              params = params))
}
  
```


```{r}

  # Pull the index of the tuning parameter which produced the highest AUC
  best_index <- which(model_tune_list==max(model_tune_list))[1]
  
  
  
  #Train final model based on best tuning parameters
  rf_i <- h2o.randomForest(
    training_frame = training_frame,
    validation_frame = validation_frame,
    x=inputs,
    y=outputs, 
    #model_id = "rf_covType_v1",
    ntrees = params[best_index,2], 
    max_depth = params[best_index,1],
    mtries = params[best_index,3],
    stopping_metric = stopping_metric,
    #stopping_rounds = 2,
    nfolds = nfolds,
    fold_assignment = "Stratified",
    #score_each_iteration = T,
    seed = seed
  )
  
  return(list(rf_i, h2o.performance(rf_i, validation_frame)))

```
## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
