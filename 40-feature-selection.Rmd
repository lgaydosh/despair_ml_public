---
title: "40-feature-selection"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

This notebook details the functions required to perform feature selection.  The 

```{r load libraries}
library(pacman)
pacman::p_load(tidyverse, h2o, yardstick,
               data.table, splitstackshape, #splitstackshape has stratified function
               boot, rsample)
```



```{r convenience functions}

# This function is for subsetting the data by outcome and predictors
get_outcomes_df <- function(df, outcome, predictors){
  # df: dataframe that has all data in it
  # outcome: outcome variable of interest
  # predictor_vars: predictor variables of interest
  # returns: tibble with only outcome and predictors of interest
  
  # create tibble to return
  out <- df %>%
    dplyr::select(outcome, predictors)
  
  # return tibble
  return(out)
}


##################
# Helper function to create bootstrapped models for RF models
rf_model <- function(outcome, 
                     training_frame, 
                     validation_frame,
                     nfolds,
                     hyper_params,
                     model_seed = 42){  
  # train: bootstrapped training frame sent in to be created a model with
  # outcome: predicted variable 
  # training_frame: 90-10 split from the training_df
  # validation_frame: 90-10 split from the training_df
  # nfolds: number of fold
  # hyper_params: hyper parameters for random forest
  
  
  # initialize function parameters for h2o.randomForest
  outputs <- outcome
  inputs <- setdiff(names(training_frame), outcome)
  
  #print(inputs)
  
  
  #print("before grid")
  # Launch grid search with given algorithm and parameters
  
  #h2o.rm(rf_grid, cascade = TRUE)
  h2o.removeAll()
  
  rf_grid <- h2o.grid("randomForest", x = inputs, y = outputs,
                      grid_id = "rf_grid",
                      training_frame = as.h2o(training_frame),
                      model_id = "rf_covType_v1",
                      stopping_rounds = 2,
                      nfolds = nfolds,
                      fold_assignment = "Stratified",
                      score_each_iteration = T,
                      seed = model_seed,
                      
                      hyper_params = hyper_params)
  
  # h2o.grid(): Starts a new grid search parameterized by
  # algorithm: Name of algorithm to use in grid search (gbm, randomForest, kmeans, glm, deeplearning, naivebayes, pca).
  # x: A vector containing the names or indices of the predictor variables to use in building the model.
  # y: The name or column index of the response variable in the data. The response must be either a numeric or a categorical/factor variable. If the response is numeric, then a regression model will be trained, otherwise it will train a classification model.
  # grid_id: (Optional) ID for resulting grid search. If it is not specified then it is autogenerated.
  # training_frame: Id of the training data frame.
  # ...: arguments describing parameters to use with algorithm (i.e., x, y, training_frame). Look at the specific algorithm - h2o.gbm, h2o.glm, h2o.kmeans, h2o.deepLearning - for available parameters.
  # hyper_params: List of lists of hyper parameters (i.e., list(ntrees=c(1,2), max_depth=c(5,7))).
  
  
  
  # Get the grid results, sorted by validation AUC
  rf_gridperf <- h2o.getGrid(grid_id = "rf_grid",
                             sort_by = "auc",
                             decreasing = TRUE)
  
  # Grab the top rf model, chosen by validation AUC
  best_rf <- h2o.getModel(rf_gridperf@model_ids[[1]])
  
  
  # return model and its performance on validation_frame
  return(list(best_rf, get_performance(validation_frame, best_rf, outcome = outputs)))
}



#####################
# Helper function to create bootstrapped models for Lasso 
lasso_model <- function(outcome, 
                        training_frame, 
                        validation_frame, 
                        nfolds,
                        hyper_params,
                        model_seed = 42){  
  # train: bootstrapped training frame sent in to be created a model with
  # outcome: predicted variable 
  # training_frame: 90-10 split from the training_df
  # validation_frame: 90-10 split from the training_df
  # nfolds: number of fold
  
  
  #initialize function parameters for h2o.randomForest
  outputs <- outcome
  inputs <- setdiff(names(training_frame), outcome)
  
  h2o.removeAll()
  
  # Launch grid search with given algorithm and parameters
  lasso_grid <- h2o.grid("glm", x = inputs, y = outputs,
                         grid_id = "lasso_grid",
                         training_frame = as.h2o(training_frame),
                         lambda_search = TRUE,
                         family = "binomial",
                         nfolds = nfolds,
                         fold_assignment = "Stratified",
                         seed = model_seed,
                         
                         hyper_params = hyper_params)
  
  # h2o.grid(): Starts a new grid search parameterized by
  # algorithm: Name of algorithm to use in grid search (gbm, randomForest, kmeans, glm, deeplearning, naivebayes, pca).
  # x: A vector containing the names or indices of the predictor variables to use in building the model.
  # y: The name or column index of the response variable in the data. The response must be either a numeric or a categorical/factor variable. If the response is numeric, then a regression model will be trained, otherwise it will train a classification model.
  # grid_id: (Optional) ID for resulting grid search. If it is not specified then it is autogenerated.
  # training_frame: Id of the training data frame.
  # ...: arguments describing parameters to use with algorithm (i.e., x, y, training_frame). Look at the specific algorithm - h2o.gbm, h2o.glm, h2o.kmeans, h2o.deepLearning - for available parameters.
  # hyper_params: List of lists of hyper parameters (i.e., list(ntrees=c(1,2), max_depth=c(5,7))).
  
  
  # Get the grid results, sorted by validation AUC
  lasso_gridperf <- h2o.getGrid(grid_id = "lasso_grid",
                                sort_by = "auc",
                                decreasing = TRUE)
  
  # Grab the top rf model, chosen by validation AUC
  best_lasso <- h2o.getModel(lasso_gridperf@model_ids[[1]])
  
  #print(str(best_lasso))
  #print(best_lasso@model$coefficients_table$names)
  
  
  # another way to both have lambda search and a line of code for cv metrics:
  # lasso_i <- h2o.glm(
  #   x=inputs,
  #   y=outputs, 
  #   training_frame = training_frame,
  #   lambda_search = TRUE,
  #   family = "binomial",
  #   alpha = 1,
  #   nfolds = nfolds,
  #   fold_assignment = "Stratified",
  #   seed = model_seed
  # )
  
  
  # return model and its performance on validation_frame
  return(list(best_lasso, get_performance(validation_frame, best_lasso, outcome = outputs)))
  
}




############################
# Get names of variables for top n predictors of random forest
# This should be called in the for loop after building the model on each bootstrap. 
# Save the result in a list - see example below
get_top_n_names <- function(model_h2o, top_n){
  # model_h2o: The h2o model object
  # top_n: The number of predictors to look at
  #       if this is -1, it gets all predictors
  # return: data frame of variable importance
  
  if(top_n == -1){
    n_all_params <- model_h2o@model$names %>% length()
    out <- h2o.varimp(model_h2o)[1:n_all_params,1] %>% 
      as.data.frame() %>% 
      mutate_all(as.character)
    
    # add row_index
    out <- out %>% mutate(placement = as.numeric(rownames(out)))
    
    # rename the period as variable name
    names(out) <- c("variable", "placement")
    
    # return top_n predictors 
    return(out)
  }
  
  
  # get names for top_n predictors
  out <- h2o.varimp(model_h2o)[1:top_n,1] %>% 
    as.data.frame() %>% 
    mutate_all(as.character)
  
  # add row_index
  out <- out %>% mutate(placement = as.numeric(rownames(out)))
  
  # rename the period as variable name
  names(out) <- c("variable", "placement")
  
  # return top_n predictors 
  return(out)
}

# Example
# h2o_model_list[[i]] <- h2o_model %>% get_top_n_names(top_n = 50) %>% as_tibble()


######################
# Convenience function to select from either "RF" or "Lasso"; seperate functions can be found in 41-modeling-rf and 42-modeling-lasso.
## return : a list of three lists: 
#          list 1 - all models created
#          list 2 - all model performances
#          list 3 - all dataframes of variable importances with the top_n
model_feature_selection <- function(model_type,
                                    training_frame,
                                    validation_frame,
                                    n = 2,
                                    k = 5,
                                    hyper_params,
                                    outcome,
                                    top_n = 20,
                                    seed = 42){
  # training_df : training data frame sent in after initial 70-30 split
  # n : n number of bootstrapped
  # k : k folds in cross-validation
  # lambda_tune : tuning parameters to test for lambda
  # hyper_params : hyperparams for random forest 
  # outcome : name of the outcome variable
  # top_n : top n important variables to be extracted from the models generated
  
  # print("model_feature_selection")
  
  straps <- training_frame %>% bootstraps(times = n, strata = outcome)
  
  combined_models <- list()
  combined_performances <- list()
  variable_imps <- list()
  
  
  for(i in 1:length(straps$splits)){
    
    # if (model_type == "RF"){
    #   results <- as.data.frame(straps$splits[[i]])%>%
    #     rf_model(outcome = outcome,
    #                   training_frame = training_frame,
    #                   validation_frame = validation_frame,
    #                   # max_depths_tune = max_depths_tune, 
    #                   # ntrees_tune = ntree_tune,
    #                   hyper_params = hyper_params,
    #                   nfolds = k,
    #                   model_seed = seed)
    # }
    
    boot_training_frame <- straps$splits[[i]] %>% as.data.frame() # bootstrapped training frame
    
    
    if (model_type == "RF"){
      #print("for loop")
      results <-  rf_model(outcome = outcome,
                           training_frame = boot_training_frame,
                           validation_frame = validation_frame,
                           hyper_params = hyper_params,
                           nfolds = k,
                           model_seed = seed)
    }
    
    else if (model_type == "Lasso"){
      results <- lasso_model(outcome = outcome,
                             training_frame = boot_training_frame,
                             validation_frame = validation_frame,
                             hyper_params = hyper_params,
                             nfolds = k,
                             model_seed = seed)      
    }
    
    combined_models[[i]] <- results[[1]]
    combined_performances[[i]] <- results[[2]]
    variable_imps[[i]] <- get_top_n_names(results[[1]], -1)
    
  }
  
  
  return(list(combined_models, combined_performances, variable_imps))
  
}

######################
# This function returns performance of a dataset (e.g., validation or test set) using the threshold from the training set.  For more use-case
# information, see 53-modeling-imbalances.Rmd.
## returns : a one-row tibble of 25 performance metrics
get_performance <- function(test_data, test_model, thresh_met = 'f1', thresh_val=NULL, outcome='h5mn8'){
  
  # test_data: tibble of test data without aids.  Must contain the true values of the outcome
  # test_model: h2o model of interest
  # thresh_met: string indicating threshold by which the prediction probabilities will be classified (default 'f1').
  #             Must be present in h2o metrics.
  # thresh_val: value by which the predictions probabilities will be classified (default NULL).  Leave NULL if you want this method to extract
  #             the threshold from the model you passed in.
  # outcome: string indicating *binary* outcome of interest
  # returns: 1-row tibble of 25 metric values as calculated by yardstick
  
  #if the metric_thresh is null, that means we need to get it from the test model
  if(is.null(thresh_val)){
    
    # get parameters for computing threshold of interest
    met_name = str_c('max ', thresh_met)
    model_metnames <- test_model@model$training_metrics@metrics$max_criteria_and_metric_scores$metric
    model_threshes <- test_model@model$training_metrics@metrics$max_criteria_and_metric_scores$threshold
    
    #Get the threshold of maximum metric of interest
    thresh_val <-model_threshes[which(model_metnames==met_name)]
    
  }
  
  #get actual column
  act <- test_data %>%
    dplyr::select(outcome) %>%
    dplyr::rename(actual = outcome)
  
  #get predictions - this returns predictions according to the training f1 score
  #releveling is done because yardstick wants to see the first level as the level of interest.
  preds <- h2o.predict(test_model, as.h2o(test_data)) %>% 
    as_tibble() %>%
    dplyr::select(-predict) %>%
    mutate(predict = as.factor(as.numeric(p1>thresh_val))) %>%
    bind_cols(act) %>%
    mutate(predict = fct_expand(predict, "0", "1"), predict = fct_relevel(predict, "1", "0")) %>% #make sure we always have the right levels available (e.g., if predict only predicts 0)
    mutate(actual = fct_expand(actual, "0", "1"), actual = fct_relevel(actual, "1", "0"))
  
  #some counting for class counts
  cls_cnt <- preds %>%
    group_by(actual) %>%
    summarise(cnt=n()) %>%
    ungroup() %>%
    as_tibble()
  
  #generate metrics of interest as tibble
  calc_metrics <- conf_mat(preds, truth=actual, estimate=predict) %>% summary() %>%
    dplyr::select(-.estimator) %>%
    #dplyr::rename(flscore = f_meas) %>%
    pivot_wider(names_from=.metric, values_from=.estimate)
  
  #add some rows for raw values and then calculate using these values
  calc_metrics <- calc_metrics %>%
    mutate(no_n = filter(cls_cnt, actual==0) %>% pull(cnt)) %>%
    mutate(no_p = filter(cls_cnt, actual==1) %>% pull(cnt)) %>%
    mutate(tns = spec * no_n) %>%
    mutate(fps = no_n - tns) %>%
    mutate(tps = sens * no_p) %>% 
    mutate(fns = no_p - tps) %>%
    mutate(mpce = (fps/no_n + fns/no_p)/2) %>%
    mutate(err_rate = (fps+fns)/(no_n+no_p)) %>%
    mutate(roc_auc = roc_auc(preds, actual, p1) %>% pull(.estimate)) %>%
    mutate(pr_auc = pr_auc(preds, actual, p1) %>% pull(.estimate)) %>%
    mutate(log_loss = mn_log_loss(preds, actual, p1) %>% pull(.estimate)) %>% 
    mutate(rmse = rmse(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate)) %>%
    mutate(mae = mae(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate))
  
  return(calc_metrics)
}

```


```{r permute columns function}
######################
# Permute a single column of the data frame
permute_column <- function(df, column){
  # df: input data frame
  # column: column to permute
  
  df[,column] <- df %>% pull(column) %>% sample()
  return(df)
}

# Permute all columns (not outcome)
permute_columns <- function(df, outcome){
  # df: input data frame
  # outcome: outcome variable (not permuted)
  
  predictor_names <- df %>% select(-outcome) %>% names()
  
  df_permute_list <- predictor_names %>% map(permute_column, df = df)
  
  names(df_permute_list) <- predictor_names
  
  return(df_permute_list)
}
```

```{r plot permutation importance}
######################
# plot the variable importance
plot_permute_var_imp <- function(df_permute_var_imp, metric){
  # df_permute_var_imp: data frame of permutation metrics from permute_var_imp()
  # metric: metric to plot, NOT passed as a string
  # return: ggplot bar graph of permutation importance
  
  # create dataframe we can use for ggplot (mostly to grab the names below)
  plot_df <- df_permute_var_imp %>% 
    group_by(variable) %>% 
    summarise_all(mean) %>% 
    select(variable, {{metric}})
  
  # Plot the metric
  plot_df %>% 
    ggplot() + 
    ylab(str_c(names(plot_df)[2], " ", "(True - Permuted)")) +
    xlab("Variable") +
    ggtitle("Permutation Importance") +
    geom_bar(aes(x = reorder(variable, {{metric}}), 
                 y = {{metric}}), 
             stat = "identity",
             fill = "light blue") +
    coord_flip() +
    theme_dark()
}

```

```{r permutation variable importance function}
# Note, make sure that factors are factors!
permute_var_imp <- function(df_predict, outcome, model_h2o){
  # df_predict: data to predict
  # outcome: outcome of interest
  # model_h2o: h2o model object
  # return: data frame of permutation importances
  
  # make permuted data frames h2o objects
  permute_list_h2o <- permute_columns(df = df_predict, outcome = outcome)
  
  # make prediction frame h2o object
  df_predict_h2o <- df_predict %>% as.h2o()
  
  # predict data for model
  df_predictions <- h2o.predict(object = model_h2o, newdata = df_predict_h2o) 
  
  # Get metrics from unpermuted data
  true_metric_vals <- get_performance(test_data = df_predict, test_model = model_h2o, outcome = outcome)
  
  # get rmse for permuted data sets
  permuted_models_metric <- permute_list_h2o %>% 
    map(~get_performance(test_data = .x, test_model = model_h2o, outcome = outcome)) 
  
  # calculate permutation importance (dif. between permuted and non-permuted predictions)
  out <- permuted_models_metric %>%
    map_df(function(permute_mets = .x, real_mets = true_metric_vals) real_mets - permute_mets) %>% 
    mutate(variable = names(permute_list_h2o)) %>% 
    select(variable, everything())
  
  return(out)
}

# example dataset
# mtcars2 <- mtcars %>% mutate(vs = factor(vs))

# example 1
# rf_h2o <- h2o.randomForest(x = c("mpg", "cyl", "disp"), 
#                            y = "vs", 
#                            training_frame = as.h2o(mtcars2))

# permute_var_imp(df_predict = mtcars2, outcome = "vs", model_h2o = rf_h2o)

# example 2
# glm_h2o <- h2o.glm(x = c("mpg", "cyl", "disp"),
#                    y = "vs", 
#                    training_frame = as.h2o(mtcars2), 
#                    family = "binomial")

# permute_var_imp(df_predict = mtcars2, outcome = "vs", model_h2o = glm_h2o)
```

```{r permutation variable importance bootstrap}

# bootstrap permutation variable importance
boot_permute_var_imp <- function(df_predict, outcome, model_h2o, n){
  # df_predict: data set to bootstrap
  # outcome: outcome of interest
  # model_h2o: h2o model object
  # n: number of bootstraps
  # return: data frame with all bootstrapped permutation importances
  
  # bootstrap the data
  b_df <- bootstraps(df_predict, times = n)
  
  # pull the bootstrap samples
  df_predict_list <- b_df$splits %>% map(rsample::training)

  # compute permutation importance
  out <- df_predict_list %>% 
    map_df(~permute_var_imp(df_predict = .x, 
                         outcome = outcome, 
                         model_h2o = model_h2o))
  
  # return df of bootstrapped permutation importance
  return(out)
}


# Example 1
# rf_h2o <- h2o.randomForest(x = c("mpg", "cyl", "disp"), 
#                            y = "vs", 
#                            training_frame = as.h2o(mtcars2))

# x <- boot_permute_var_imp(df_predict = mtcars2, outcome = "vs", model_h2o = rf_h2o, n = 2)

# Example 2
# glm_h2o <- h2o.glm(x = c("mpg", "cyl", "disp"),
#                    y = "vs", 
#                    training_frame = as.h2o(mtcars2), 
#                    family = "binomial")

# x <- boot_permute_var_imp(df_predict = mtcars2, outcome = "vs", model_h2o = glm_h2o, n = 2)

# Example 3
# Get permutation importance for two models
# model_list <- list(rf_mod = rf_h2o, glm_mod = glm_h2o)
# y <- model_list %>% 
#   map(~boot_permute_var_imp(df_predict = mtcars2, outcome = "vs", model_h2o = .x, n = 2))
# y$rf_mod
# y$glm_mod

# Look at graph
# plot_permute_var_imp(x, pr_auc)
# plot_permute_var_imp(y$rf_mod, accuracy)
# plot_permute_var_imp(y$glm_mod, roc_auc)
```





