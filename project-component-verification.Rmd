---
title: "project-component-verification"
output: 
  html_notebook:
    toc: true
    toc_float: true
    theme: lumen
    code_folding: hide
editor_options: 
  chunk_output_type: inline
---

# Verifying project components

The purpose of this document is to verify the behavior of certain parts of the project and determine if they're working similarly on everyone's machines.  Make sure you run `10-import-data.Rmd` first.

```{r libraries used}
pacman::p_load(tidyverse, yardstick, janitor, h2o)
```

```{r constants and settings}
seed = 2435
set.seed(seed)

options(yardstick.event_first = TRUE)
```

```{r}
#load waves, join, and select desired aids and features of data
wave_data <- load_waves(1:5)
```

```{r join all waves}
full_dataset <- get_working_dataset_full(wave_data, join_type = 'full')
```

```{r}
# set outcome variable of interest
outcome = 'h5mn8'

# get the aids that you want
inner_aids <- get_inner(list(wave_data[[1]], wave_data[[3]], wave_data[[4]], wave_data[[5]]))

# use the features and ids that you want to select out what you want
working_ds <- full_dataset %>%
                filter(aid %in% inner_aids) %>%
                select(aid, predictor_list, outcome)

```

## Helper functions

```{r amended rf modeler}
# Generates a list of lists corresponding to: $mods: models built, $perfs: performance on validation set, $params: hyperparameters supplied by user
model_rf <- function(train_frame, valid_frame, outcome, max_depths_tune, ntrees_tune,
                          mtries_tune=-1, nfolds=5,
                          stopping_metric='AUTO', stopping_tolerance = 1e-3,
                          balance_classes= FALSE, seed=2435){  
  
  # if you have a validation frame, this means that you want to ensure same comparison across all models.  Otherwise, bootstrapping
  if(!is.null(valid_frame)){
    training_frame <- as.h2o(train_frame)
    validation_frame <- as.h2o(valid_frame)
  }
  else{
    splits <- train_frame %>% rsample::initial_split(strat=outcome, prop=0.9)
    training_frame <- as.h2o(rsample::training(splits))
    validation_frame <- as.h2o(rsample::testing(splits))
  }
  
  # Create all combinations of tuning parameters
  params <- expand.grid(max_depth = max_depths_tune, ntrees = ntrees_tune, mtries=mtries_tune)
  models <- c()
  performances <- c()
  
  #initialize function parameters for h2o.randomForest
  outputs <- outcome
  inputs <- setdiff(names(training_frame), outcome)
  
  #Training models on all combinations of parameters
  for(i in 1:nrow(params)){
    rf_i <- h2o.randomForest(
      training_frame = training_frame,
      x= inputs,
      y= outputs, 
      #model_id = "rf_covType_v1",
      ntrees = params[i,][[2]],
      max_depth = params[i,][[1]],
      mtries = params[i,][[3]],
      stopping_metric = stopping_metric,
      stopping_tolerance = stopping_tolerance,
      balance_classes = balance_classes,
      stopping_rounds = 2,
      nfolds = nfolds,
      fold_assignment = "Stratified",
      #score_each_iteration = T,
      seed = seed
    )
    
    #Save the model and performance on validation set to assess full model performance
    models <- models %>%
      append(rf_i)
    
    performances <- performances %>%
      append(h2o.performance(rf_i, validation_frame))
    
  }
  
  return(list(mods = models,
              perfs = performances,
              params = params))
}
  
```

```{r metric calculation function using h2o}
get_performance_h2o <- function(test_model, test_data, outcome='h5mn8'){

#get actual column
act <- test_data %>%
  select(outcome) %>%
  rename(actual = outcome)

#get predictions - this returns predictions according to the training f1 score
preds <- h2o.predict(test_model, as.h2o(test_data)) %>% 
  as_tibble() %>%
  bind_cols(act) %>%
  mutate(predict = fct_relevel(predict, "1", "0")) %>%
  mutate(actual = fct_relevel(actual, "1", "0"))

#some calcs
cls_cnt <- preds %>%
  group_by(actual) %>%
  summarise(cnt=n()) %>%
  ungroup() %>%
  as_tibble()

#generate metrics of interest as tibble
calc_metrics <- conf_mat(preds, truth=actual, estimate=predict) %>% summary() %>%
  select(-.estimator) %>%
  #dplyr::rename(flscore = .f_meas) %>%
  spread(key=.metric, value=.estimate)

#add some rows for raw values
calc_metrics <- calc_metrics %>%
  mutate(no_n = filter(cls_cnt, actual==0) %>% pull(cnt)) %>%
  mutate(no_p = filter(cls_cnt, actual==1) %>% pull(cnt)) %>%
  mutate(tns = spec * no_n) %>%
  mutate(fps = no_n - tns) %>%
  mutate(tps = sens * no_p) %>% 
  mutate(fns = no_p - tps) %>%
  mutate(mpce = (fps/no_n + fns/no_p)/2) %>%
  mutate(roc_auc = roc_auc(preds, actual, p1) %>% pull(.estimate)) %>%
  mutate(pr_auc = pr_auc(preds, actual, p1) %>% pull(.estimate)) %>%
  mutate(log_loss = mn_log_loss(preds, actual, p1) %>% pull(.estimate)) %>% 
  mutate(rmse = rmse(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate)) %>%
  mutate(mae = mae(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate))

return(calc_metrics)  
}

```


```{r}
get_performance <- function(test_data, test_model, metric_thresh, outcome='h5mn8'){
#given a model, let's predict outcomes using it

#get actual column
act <- test_data %>%
  select(outcome) %>%
  rename(actual = outcome)

#get predictions - this returns predictions according to the training f1 score
#releveling is done because yardstick wants to see the first level as the level of interest.
preds <- h2o.predict(test_model, as.h2o(test_data)) %>% 
  as_tibble() %>%
  select(-predict) %>%
  mutate(predict = as.factor(as.numeric(p1>metric_thresh))) %>%
  bind_cols(act) %>%
  mutate(predict = fct_relevel(predict, "1", "0")) %>%
  mutate(actual = fct_relevel(actual, "1", "0"))


#some calcs
cls_cnt <- preds %>%
  group_by(actual) %>%
  summarise(cnt=n()) %>%
  ungroup() %>%
  as_tibble()

#generate metrics of interest as tibble
calc_metrics <- conf_mat(preds, truth=actual, estimate=predict) %>% summary() %>%
  select(-.estimator) %>%
  #dplyr::rename(flscore = f_meas) %>%
  pivot_wider(names_from=.metric, values_from=.estimate)

#add some rows for raw values and then calculate using these values
calc_metrics <- calc_metrics %>%
  mutate(no_n = filter(cls_cnt, actual==0)  %>% pull(cnt)) %>%
  mutate(no_p = filter(cls_cnt, actual==1) %>% pull(cnt)) %>%
  mutate(tns = spec * no_n) %>%
  mutate(fps = no_n - tns) %>%
  mutate(tps = sens * no_p) %>% 
  mutate(fns = no_p - tps) %>%
  mutate(mpce = (fps/no_n + fns/no_p)/2) %>%
  mutate(roc_auc = roc_auc(preds, actual, p1) %>% pull(.estimate)) %>%
  mutate(pr_auc = pr_auc(preds, actual, p1) %>% pull(.estimate)) %>%
  mutate(log_loss = mn_log_loss(preds, actual, p1) %>% pull(.estimate)) %>% 
  mutate(rmse = rmse(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate)) %>%
  mutate(mae = mae(preds, truth=as.numeric(actual), estimate=as.numeric(predict)) %>% pull(.estimate))

return(calc_metrics)
}
```

## H2o: Why are the reported training metrics and results from `h2o.performance()` different?

In this section, we want to make sure that h2o is working as we would expect and require for this project.  I think I've figured it out.  Let's generate our regular workflow just to start out with:

```{r}
h2o.init()
```

```{r}
#create stratified split for overall training and testing
splits <- working_ds %>%
  rsample::initial_split(strat=outcome, prop=0.8)

train_set <- rsample::training(splits)
test_set <- rsample::testing(splits)

#use another stratified split to split into training vs validation
train_splits <- train_set %>% rsample::initial_split(strat=outcome, prop=0.9) 
  
# pull out training and validation frames
train_ss <- rsample::training(train_splits)
valid_ss <- rsample::testing(train_splits)
```


```{r}
#generate test model to be explored
test_results <- train_ss %>% 
  select(-aid) %>%
  model_rf(valid_ss, outcome, max_depths_tune = 20, ntrees_tune = 50, nfolds = 2, seed=3242)

test_model <- test_results$mods[[1]]
```

So, here are the outright f1 metrics of k+1th model (returned after training):

```{r}
test_model@model$training_metrics@metrics$max_criteria_and_metric_scores
```

We can verify this functionality outright by looking at the thresholds and calculating these values:

```{r}
#here are the thresholds that H2o uses:
threshes <- test_model@model$training_metrics@metrics$thresholds_and_metric_scores$threshold
f1s <- test_model@model$training_metrics@metrics$thresholds_and_metric_scores$f1
max_ind <- which(f1s == max(f1s, na.rm = TRUE))
max_f1<- threshes[max_ind]
str_c('Index of max f1: ', max_ind)
str_c('Max f1 value: ', max_f1)
```
This matches exactly what comes from h2o with the exception of the index (h2o's indices are 0-based, so the R equivalent will be one greater).  Now, let's look at what would happen if we called `h2o.performance()` on that same set:

```{r}
# Let's look at the performance.  Let's use the training_frame.
perf <- h2o.performance(test_model, newdata = as.h2o(train_ss))
perf@metrics$max_criteria_and_metric_scores
```

Oh no!  Different, which is unexpected.  Why is this?

Whelp, let's take a quick foray into the training metrics:

```{r}
test_model@model$training_metrics@metrics$description
```

Well look at that - clear as day.  "Metrics reported on Out-Of-Bag training samples".  This is documented [in H2o's issue tracker](https://0xdata.atlassian.net/projects/PUBDEV/issues/PUBDEV-5795?filter=allopenissues&orderby=priority%20DESC&keyword=performance).  It looks like it was fixed in version 3.24.0.  Sadly, I'm on version 3.18.  What does this mean?  Whenever a tree is calculated, there are OOB samples left over from the bootstrap.  The metrics are then calculated on these.  So, these training metrics on OOB samples would not match those of the full training set.

## H2o: The h2o.predict() and h2o.performance() functions seem to produce different results

Let's take a look at this.  So, using the model we just made, let's get the performance on the testing set.

```{r}
perf_test <- h2o.performance(test_model, newdata=as.h2o(test_set))
perf_test
```

OK, so we got some performance.  That's...interesting, though.  Why are we calculating a new f1 score?  Why are we computing new max metrics, because it sure seems like we should use the threshold from our training set to calculate the thresholds.  Maybe that isn't the case, and maybe they're just calculating those things in general for our general knowledge.  OK.  Let's put it to the test.

`h2o.predict()` actually uses the threshold from the training data in its predictions.  Let's see if the confusion matrix from those predictions actually matches that:

```{r}
preds <- h2o.predict(test_model, newdata=as.h2o(test_set))
```

The preds environment will have 3 columns: `predict`, `p0`, and `p1`.  The `predict` column is a conclusion from `p0` and `p1` based on the threshold (from the training set) that it applied.  Let's see if it matches the results from h2o.performance.
```{r}
test_res <- get_performance_h2o(test_model, test_set)
test_res
```
The AUCs for both of these should be the same, because they're independent of which classes each example is assigned to (due to the thresholding sweep):
```{r}
str_c('AUC from h2o: ', perf_test@metrics$AUC)
str_c('Manual AUC: ', test_res$roc_auc)
```
Same.  Good.

And the confusion matrices?  From h2o:
```{r}
perf_test@metrics$cm
```

```{r}
str_c(test_res$tns, test_res$fps, sep='     ')
str_c(test_res$fns, test_res$tps, sep='     ')
```

Different, which is unfortunate.  This is because different thresholds result in different classifications of positive and negative examples.

## Resultant workflow
Because of all of these differences, we're going to do three things:

1. Get the best threshold from the training set metrics.
2. Use this threshold directly on the predictions obtained from predict()
3. Make the cm calculations thusly.  This is shown below:

```{r}
train_perf <- h2o.performance(test_model, train=TRUE)
max_f1_thresh <- h2o.find_threshold_by_max_metric(train_perf, 'f1')
max_f2_thresh <- h2o.find_threshold_by_max_metric(train_perf, 'f2')
```


```{r}
f1_preds <- test_set %>%
  get_performance(test_model, max_f1_thresh)

f2_preds <- test_set %>%
  get_performance(test_model, max_f2_thresh)

```
```{r}
bind_rows(mutate(f1_preds, met_name='f1'),
          mutate(f2_preds, met_name='f2')) %>%
  select(met_name, everything())
```
I decided here to look at the f2 score because it will care a lot less about the number of negatives.  It seems to be an accurate measure in assessing the performance of this model.

# Other points of consternation: Subject-Wave Membership

Should we be concerned that it appears that not all of the participants are present in Wave 1?  Consider the following:

Let's get the aids of all of the waves individually.
```{r}
aids_1 <- wave_data[[1]] %>% pull(aid)
aids_2 <- wave_data[[2]] %>% pull(aid)
aids_3 <- wave_data[[3]] %>% pull(aid)
aids_4 <- wave_data[[4]] %>% pull(aid)
aids_5 <- wave_data[[5]] %>% pull(aid)
```

Alright.  Now, let's do some setdiffs to make sure that all of the elements that are in whatever other wave are in wave 1.  `setdiff(x,y)` returns the elements of `x` that are *not* in `y`.

```{r}
setdiff(aids_2, aids_1)
```
whoops!

```{r}
setdiff(aids_3, aids_1)
```
WHOOPS

```{r}
setdiff(aids_4, aids_1)
```
yay!

```{r}
setdiff(aids_5, aids_1)
```
yay!

