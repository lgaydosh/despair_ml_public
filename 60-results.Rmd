---
title: "60-results"
output: html_document
---

The purpose of this document is to explore the results of the models built in the 50 series.  The following define the main functions which will be used in the 60 series for further investigation and the 70 series to understand the results of the analysis.

```{r load libraries}
library(pacman)
pacman::p_load(tidyverse)
```

## Helper functions
In general, one shouldn't use these functions directly.  They're helpers to perform the functionality of the public functions.


## Public functions
The following functions are available for usage to evaluate the performance of the model.

```{r metric set from models}
# This function returns the metrics for a set of models on an input dataframe
get_metric_set_from_models <- function(df, model_list){
  
  # df: Training or testing frame
  # model_list: list of h2o model objects
  # return: tibble with all applicable metrics for each model tested on df

  metric_set <- model_list %>%
  map(get_performance, test_data = df) %>% 
  get_metric_set_from_perfs()
  
  # return metric set
  return(metric_set)
}
```

```{r metric set from performance metrics tibbles}
# This function returns the metrics for a set of models from the performance metrics
get_metric_set_from_perfs <- function(perf_list){
  
  #perf_list: List of performance metrics generated by get_performance function
  #return: an aggregated tibble of all of the metrics in the list
  
  metric_set <- perf_list %>% 
    purrr::reduce(rbind) %>%
    mutate(model = factor(1:length(perf_list))) %>% 
    dplyr::select(model, everything())
  
  # return metric set
  return(metric_set)
  
}
```

```{r metric comparison ploting functions}
# This function returns a visualization of all the metrics for the models being compared

plot_metric_set <- function(df_metrics, x_text_angle = 90, plot_title = "Model Comparison Plot"){
  # df_metrics: metrics from get_metric_set_* functions
  # return: ggplot object (scatterplot) comparing all models across all applicable metrics
  
  # pivot metrics to put in tidy format and plot
  df_metrics %>% 
    pivot_longer(cols = -model,
                 names_to = "metric",
                 values_to = "metric_val") %>% 
    drop_na() %>% 
    ggplot() +
    geom_point(aes(x = metric, y = metric_val, color = model), size = 3) +
    xlab("Metric") +
    ylab("Metric Value") +
    ggtitle(plot_title) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = x_text_angle, size = 11),
          axis.text.y = element_text(size = 11))
}

```

```{r plot metrics from list of h2o models}
plot_h2o_metric_set <- function(df, model_list, ...){
  # df: data frome to get metrics for (likely testing df)
  # model_list: list of models to get metrics for and plot
  # ...: parameters passed to plot_metric_set
  model_list %>% 
    map_df(~get_metric_set_from_models(df = df, model_list = model_list)) %>% 
    plot_metric_set(...)
}
```

```{r feature selection comparison}
compare_feature_select <- function(df_joined = joined_results){
  # df_joined_results: output from joined results df
  df_joined %>% 
    mutate(median_dif_rf_lasso = median_rank.rf - median_rank.lasso) %>% 
    group_by(predictor) %>%
    ggplot() +
    geom_point(aes(x = fct_reorder(predictor, -median_dif_rf_lasso), y = median_dif_rf_lasso))+
    scale_x_discrete(expand = c(-0.001,0))+ 
    labs(y = "Placements", x = "Variables")+
    coord_flip()
}

# joined_results %>% slice(1:20) %>%  compare_feature_select()
```

