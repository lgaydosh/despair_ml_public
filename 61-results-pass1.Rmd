---
title: "61-results-pass1"
output: html_document
---

The purpose of this document is to explore the results of the models built in the 50 series.
```{r load libraries}
# These are libraries needed to run the code below
```

```{r convenience functions}
# This function returns the metric from an h20 model fit
get_metrics <- function(df, h2o_model){
  # df: training or testing frame
  # h2o_model: h2o model fit to use
  # returns: tibble of metrics
  
  h2o_df <- df %>% as.h2o()
  
  # calculate result
  result <- h2o.performance(model = h2o_model, newdata = h2o_df)
  
  # Quick convenience function to check null values
  # Note, intentionally not using if_else() to avoid strictness of if_else
  check_null <- function(...) ifelse(is.null(...), NA, ...)
  
  # get metrics of interest
  metrics <- tibble(auc = check_null(result@metrics$AUC), 
                    mse = check_null(result@metrics$MSE),
                    rmse = check_null(result@metrics$RMSE),
                    r2 = check_null(result@metrics$r2),
                    logloss = check_null(result@metrics$logloss), 
                    mean_resid_deviance = check_null(result@metrics$mean_residual_deviance),
                    mae = check_null(result@metrics$mae))
  
  # return metric tibble
  return(metrics)
  
}

get_h2o_metric_set <- function(df, model_list){
  # df: Training or testing frame
  # model_list: list of h2o model objects
  # return: tibble with all applicable metrics for each model tested on df

  # get metric set
  metric_set <- model_list %>%
  map_df(get_metrics, df = df) %>% 
  mutate(model = factor(1:length(model_list))) %>% 
  select(model, everything())
   
  # return metric set
  return(metric_set)
}

plot_h2o_metric_set <- function(df, model_list){
  # df: Training or testing frame
  # model_list: list of h2o model objects
  # return: ggplot object (scatterplot) comparing all models across all applicable metrics

  # Get metrics
  df_metrics <- get_h2o_metric_set(df, model_list)
  
  # pivot metrics to put in tidy format and plot
  df_metrics %>% 
    pivot_longer(cols = -model,
                 names_to = "metric",
                 values_to = "metric_val") %>% 
    drop_na() %>% 
    ggplot() +
    geom_point(aes(x = metric, y = metric_val, color = model), size = 3) +
    xlab("Metric") +
    ylab("Metric Value") +
    ggtitle("Model Comparison Plot") +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.text = element_text(size = 11))
}
```

```{r explore variable importance for suicide}
# This will give you a data frame of top predictors

top_preds_outcome <- suicide_out[[3]] %>%
  get_total_placement() %>% 
  mutate(avg_place = total /2,
         top_20 = if_else(avg_place <= 20, "Top 20", "Not Top 20"))

# Plot top predictors
top_preds_outcome %>% 
  arrange(desc(total)) %>%
  ggplot(aes(x = reorder(variable, -avg_place), y = avg_place)) +
  geom_col(aes(fill = top_20)) +
  geom_hline(yintercept = 20, color = "yellow") +
  xlab("Predictor Variable") + 
  ylab("Average Importance") +
  ggtitle("Variable Importance based on Bootstrap") +
  coord_flip() +
  theme_dark()

```

```{r final random forest model for suicide ideation}
outcome <- c("h5mn8")

# final features are the top variables extracted from the previous chunk
final_features <- top_preds_outcome$variable
tuning_param <- expand.grid(max_depth = c(7,8,9 ,10), ntrees = c(100,150,200,250))
model_tune_list <- rep(NA, nrow(tuning_param))

for(i in 1:nrow(tuning_param)){
  rf_i <- h2o.randomForest(
    training_frame = as.h2o(training_df),
    x=final_features,
    y= outcome, 
    model_id = "rf_covType_v1",
    max_depth = tuning_param[i,][[1]],
    ntrees = tuning_param[i,][[2]], 
    stopping_rounds = 2,
    nfolds = 5,
    fold_assignment = "Stratified",
    score_each_iteration = T,
    seed = 42
  )
  
  result <- h2o.performance(rf_i, as.h2o(training_df))
  
  model_tune_list[i] <- result@metrics$AUC
}

best_index <- which(model_tune_list==max(model_tune_list))[1]


rf_final_model <- h2o.randomForest(
  training_frame = as.h2o(training_df),
  x=final_features,
  y=outcome, 
  model_id = "rf_covType_v1",
  ntrees = tuning_param[best_index,][[2]], 
  max_depth = tuning_param[best_index,][[1]],
  stopping_rounds = 2,
  nfolds = 5,
  fold_assignment = "Stratified",
  score_each_iteration = T,
  seed = 42
)
```

```{r explore results}
# get results for testing frame
results <- testing_df %>% 
  get_metrics(h2o_model = rf_final_model)
```

```{r plot model results}
# Plot model results
# Note, to actually make the comparison, just include the glm_model in the list
plot_h2o_metric_set(df = testing_df, model_list = list(rf_final_model))
```

