---
title: "71-experiments-rf"
output:
  html_notebook:
    code_folding: hide
    theme: lumen
    toc: yes
    toc_float: yes
    toc_depth: 4
  html_document:
    df_print: paged
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r source files}
source("function_import.R")
```

**Purpose.** In this work, we will explore the relation between identified measures of despair of interest (e.g., personality measures of self-consciousness, individual and composite item scores from the CES-D assessment) and descriptors of diseases of despair.  We will achieve this goal through modeling the outcomes based on the included predictors, and robustly assess the importance of the included features in predicting the outcomes via bootstrapping.  We will use two well-known machine learning models, random forests and LASSO, which are both frequently used to measure the relative importance of the predictors included in the models.  Lastly, we'll generate trained and tuned models using this reduced feature set which can be used by others wish to predict the identified outcomes.

**Subject inclusion.** For this investigation, we will omit the entirety of Wave 2.  This is commonly done in analyses of AddHealth data due the design of the original study.  Otherwise, our dataset will include only subjects who have predictor and outcome data in _all_ of the waves.

**Outcome variables.** In this experiment, we assess _suicidal ideation_ at Wave 5.  

**Predictor variables.** The predictors for these models are hand-picked, and based on previous work, relevance, and subject matter expertise. The set of predictors and the set of outcomes are disjoint.  Predictors from Waves 1-4 (excluding Wave 2, see above) are included, and will be detailed in the following analysis.

```{r load libraries, include=FALSE}
# Use pacman, which forces an install if the library isn't present on the running machine
if (!require("pacman")) install.packages("pacman")
#pacman::p_install(plotly)
pacman::p_load(tidyverse, h2o, furrr, tictoc)

```

```{r initializations, include=FALSE}
port_no <- start_h2o()
h2o.no_progress()
future::plan(multisession, workers=11)
```

```{r seeds for reproducibility}
seed= 9384
h2o_seed=-1
set.seed(seed)
```

# Dataset generation

The predictors we will be using will be the the variable `predictor_list` loaded from `10-import-data.Rmd` file. These initial set of predictors will be based of the list of variables that describe anxiety, depression, and optimism.

```{r load raw data and formulate dataset, warning=TRUE, message=TRUE}
## set outcome variable of interest
outcome = 'h5mn8'
filebase = '/scratch/p_gaydosh_dsi'
results_directory = str_c(filebase, '/DSI/', outcome, '/', Sys.Date(), '/rf')

#create data in specified form
dataset_list <- generate_datasets(outcome, binarize=FALSE, filebase=filebase, seed_val=seed)

#parse out dataset components
wave_data <- dataset_list$wave_data
full_dataset <- dataset_list$full_dataset
ds_raw <- dataset_list$ds_raw_outcome
ds <- dataset_list$ds_final

#ml splits of the data
training_df <- dataset_list$training_df
validation_df <- dataset_list$validation_df
testing_df <- dataset_list$testing_df

```

## RF model
The RF models are chosen based on a grid search using the following the parameters: 

  - max depth: maximum depth allowed for a single tree in the RF  
  - number of trees: maximum number of trees allowed in the RF  
  - mtries: the number of columns sampled for each tree split  
  - min_rows: the minimum number of rows required to split the internal node
  - balance classes: whether to balance the classes or not
  - stopping_metric: metric which results in early stopping of training of the model
  - categorical encoding: use one hot encoding to create straightforward comparison with LASSO

```{r feature selection rf, include=FALSE}
tic()
# Spans of hyper parameters for random forest
rf_params <- list(max_depth = 50,
                  ntrees = 150,
                  mtries = c(-1, 20, by = 5),
                  min_rows = c(2, 20, by = 5)
                  #stopping_rounds = 4,
                  #balance_classes = c(TRUE, FALSE),
                  #stopping_metric = 'AUCPR'
                  #categorical_encoding = 'one_hot_explicit')
                  )

# rf_params <- list(max_depth = c(20, 50),
#                   balance_classes = TRUE,
#                   categorical_encoding= 'one_hot_explicit')

# define number of bootstraps
n_boot = 500

boots_rf <- model_feature_selection("RF",training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      outcome = outcome, n = n_boot, seed=seed, h2o_seed=h2o_seed)
toc()

```

```{r get bs seeds and performance}
bs_rf_seeds <- get_model_seeds(boots_rf$models)

bs_rf_perf <- get_metric_set_from_perfs(boots_rf$perfs) %>%
  dplyr::select(accuracy, mpce, sens, spec, ppv, npv, roc_auc, pr_auc,
                tns, tps, fns, fps, no_n, no_p,  err_rate, bal_accuracy, everything())
```


The following table displays the mean performance metrics for the bootstrapped models on the validation set, removing values for which there are NA.

```{r evaluate bootstrap model performance rf, warning=FALSE, message=FALSE}
mean_bs_rf_perf <- bs_rf_perf %>%
  summarise_if(is.numeric, mean, na.rm=TRUE) %>%
  mutate(model = 'bs_rf') %>%
  dplyr::select(model, everything())

mean_bs_rf_perf
```
As shown, the bootstrapped models tend to have high specificity but low sensitivity, indicating that there is a challenge in identifying subjects with suicidal ideation.

### Feature importances: Random Forest
#### Mean decrease in impurity (MDI)
```{r}
boot_rf_mdi <- boots_rf$mdi %>%
  get_median_placement(use_base_var = TRUE) %>%
  add_attribute_names('predictor', full_dataset) %>%
  dplyr::select(predictor, att_name, overall_rank)

head(boot_rf_mdi, 20)
```
This table returns the MDI variable importance ranks that returned from each of the bootstrapped models.

```{r fig.width = 10, fig.height = 12}
# Needs to be fixed so that axes don't overlap each other and obscure understanding
plot_placement_boxplot(boots_rf$mdi)
```

#### Permutation importance
Now, let's look at the permutation importance:

```{r get rf permutation importance, warning=FALSE, message=FALSE}
tic()
boot_rf_perm_plt <- boots_rf$models %>%
  get_aggregated_permute_imp(training_df, outcome=outcome, h2o_port=port_no)
toc()
```

```{r aggregate rf perm results}
met <- 'pr_auc'
boot_rf_perm <- boot_rf_perm_plt %>%
  get_permute_placement(metric_oi=met) %>%
  add_attribute_names('predictor', full_dataset) %>%
  dplyr::select(predictor, everything())

head(boot_rf_perm, 20)
```

#### MDI vs Permutation importance
In this step, we assess the differences generated between the different types of importances.
```{r fig.width = 16, fig.height = 14}
cbind(boot_rf_mdi[1:20,], dplyr::select(boot_rf_perm[1:20,], -all_of(met)))
```
As shown, the MDI importance suffers from imbalances due to the number of values associated with a predictor.  Because the wave ages have so many more values than the other factors, this artificially inflates their importance in MDI.  The permutation importance is more intuitive.

```{r fig.width = 10, fig.height = 12}
plot_permute_var_imp(boot_rf_perm, metric = met)
```

# Save data and models
```{r}
#data
save_var_list <- c('bs_rf_perf',
                   'mean_bs_rf_perf',
                   'bs_rf_seeds',
                   'boot_rf_perm_plt',
                   'boot_rf_mdi')

dir_create(results_directory)
save(list=save_var_list, file=str_c(results_directory, '/', 'experimental_results.RData'))
```

# Other cleanup
```{r shutdown h2o instance}
h2o.shutdown(FALSE)
```



