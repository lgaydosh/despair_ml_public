---
title: "43-evaluate-h2o-grid"
output: html_notebook
---

# 43-evaluate-h2o-grid

The purpose of this notebook is to investigate the behavior of `h2o.grid`.  A current strange behavior is that h2o.grid, when called on multiple different datasets, appears to return the same results every single time, even though the dataset is different.  This notebook will use some of the existing functions in 40 and ensure that their behavior is as expected.

```{r}
library(pacman)
pacman::p_load(tidyverse, h2o)
```


```{r load the data}
## load waves and join them
wave_data <- load_waves(1:5)
full_dataset <- get_working_dataset_full(wave_data, join_type = 'full')

## set outcome variable of interest
outcome = 'h5mn8'

## get the aids that you want
inner_aids <- get_inner(list(wave_data[[1]], wave_data[[3]], wave_data[[4]], wave_data[[5]]))
```

```{r create dataset of interest}
## use the features and ids that you want to select out what you want
working_ds <- mini_dataset %>%
  filter(aid %in% inner_aids) %>%
  select(aid, predictor_list, outcome) %>%
  remove_subjects_not_in_wave1() %>%
  drop_na(outcome)

## split the data into what you want
data_splits <- working_ds %>%
  split_data(strat_var = outcome, ratios=c(0.7, 0.2, 0.1))

# assemble list
training_df <- data_splits$train
validation_df <- data_splits$valid
testing_df <- data_splits$test
```
```{r}
h2o.init()
```

## Grid names
```{r}
rf_params <- list(max_depth = 50, 
                  ntrees = 150,
                  mtries = c(-1,20))

lasso_params <- list(alpha = c(1))


# define number of bootstraps
n_boot = 2

suicide_rf <- model_feature_selection("RF", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      outcome = outcome, n = n_boot)

suicide_lasso <- model_feature_selection("Lasso", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = lasso_params,
                      outcome = outcome, n = n_boot)
```
This seems to work satisfactorally.  It looks like the issue was that whenever you provide a grid_name as a constant (i.e., the same between different calls to the function), it cannot append on the data based on different training data.  The `h2o.remove_all` function fixed this.  However, when you make yet another call, then sometimes, the H2o object is then missing.

We can get around this by just providing a random hash long enough that we can expect that it will not be repeated.  We appended another part, e.g. `rf` or `lasso` just to help out with the randomness.  This approach seems to fix the issue without removing information from the h2o process space.

Note that one-hot encoding (explicit) proceeds as described [here](https://groups.google.com/forum/#!topic/h2ostream/bSic0Rm8-98).

## Variable grid performance measures

The commit following 53f4cb0 (I haven't committed yet) implements new functionality to close #173, which implements a variable grid search selection metric.  Here, I test the functionality I'm about to commit.  I ran the first few code chunks of 71-suicidal to produce the dataframe I'm working with, so the chunks above may not be relevant.

Since I'm running this over and over, I keep getting h2o errors about "Cannot append new models to a grid with different training input."  To remedy this, enter `h2o.removeAll()` at the console and keep it moving.

```{r rf bootstrap test}
outcome = 'h5mn8'

rf_params <- list(max_depth = 50, 
                  ntrees = 150,
                  mtries = c(-1,20))

suicide_rf <- model_feature_selection("RF", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      selection_metric = 'auc',
                      outcome = outcome, n = 2)
```
Verified by debugger that:
- `perf_metric` is correctly set as a string when called from `model_feature_selection` for random forest.  The grid search only produces 1 result for lasso since our lasso is implemented with grid search for a unified framework with random forest.
- `selection_metric`/`perf_metric` is correctly used by h2o when is `pr_auc` and `auc`.  Other metrics that can be used are found in the model evaluation metrics list.

The following evaluation is shown for completeness for the individual model:

```{r}
rf_test_ind <- rf_model(outcome, 
                     training_frame = training_df,
                     validation_frame = validation_df,
                     nfolds = 5,
                     hyper_params = rf_params,
                     perf_metric = 'mean_per_class_error',
                     model_seed = 42)
```
Verified by debugger that:
- `perf_metric` correctly used at the model level
- `perf_metric` correctly used to choose best grid search model
- best grid search model correctly chosen.



