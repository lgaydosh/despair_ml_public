---
title: "43-evaluate-h2o-grid"
output: html_notebook
---

# 43-evaluate-h2o-grid

The purpose of this notebook is to investigate the behavior of `h2o.grid`.  A current strange behavior is that h2o.grid, when called on multiple different datasets, appears to return the same results every single time, even though the dataset is different.  This notebook will use some of the existing functions in 40 and ensure that their behavior is as expected.

```{r}
library(pacman)
pacman::p_load(tidyverse, h2o, tictoc)
```


```{r load the data}
## load waves and join them
wave_data <- load_waves(1:5)
full_dataset <- get_working_dataset_full(wave_data, join_type = 'full')

## set outcome variable of interest
outcome = 'h5mn8'

## get the aids that you want
inner_aids <- get_inner(list(wave_data[[1]], wave_data[[3]], wave_data[[4]], wave_data[[5]]))
```

```{r create dataset of interest}
## use the features and ids that you want to select out what you want
working_ds <- mini_dataset %>%
  filter(aid %in% inner_aids) %>%
  select(aid, predictor_list, outcome) %>%
  remove_subjects_not_in_wave1() %>%
  drop_na(outcome)

## split the data into what you want
data_splits <- working_ds %>%
  split_data(strat_var = outcome, ratios=c(0.7, 0.2, 0.1))

# assemble list
training_df <- data_splits$train
validation_df <- data_splits$valid
testing_df <- data_splits$test
```
```{r}
h2o.init()
```

## Grid names
```{r}
rf_params <- list(max_depth = 50, 
                  ntrees = 150,
                  mtries = c(-1,20))

lasso_params <- list(alpha = c(1))


# define number of bootstraps
n_boot = 2

suicide_rf <- model_feature_selection("RF", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      outcome = outcome, n = n_boot)

suicide_lasso <- model_feature_selection("Lasso", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = lasso_params,
                      outcome = outcome, n = n_boot)
```
This seems to work satisfactorally.  It looks like the issue was that whenever you provide a grid_name as a constant (i.e., the same between different calls to the function), it cannot append on the data based on different training data.  The `h2o.remove_all` function fixed this.  However, when you make yet another call, then sometimes, the H2o object is then missing.

We can get around this by just providing a random hash long enough that we can expect that it will not be repeated.  We appended another part, e.g. `rf` or `lasso` just to help out with the randomness.  This approach seems to fix the issue without removing information from the h2o process space.

Note that one-hot encoding (explicit) proceeds as described [here](https://groups.google.com/forum/#!topic/h2ostream/bSic0Rm8-98).

## Variable grid performance measures

The commit following 53f4cb0 (I haven't committed yet) implements new functionality to close #173, which implements a variable grid search selection metric.  Here, I test the functionality I'm about to commit.  I ran the first few code chunks of 71-suicidal to produce the dataframe I'm working with, so the chunks above may not be relevant.

Since I'm running this over and over, I keep getting h2o errors about "Cannot append new models to a grid with different training input."  To remedy this, enter `h2o.removeAll()` at the console and keep it moving.

```{r rf bootstrap test}
outcome = 'h5mn8'

rf_params <- list(max_depth = 50, 
                  ntrees = 150,
                  mtries = c(-1,20))

suicide_rf <- model_feature_selection("RF", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      selection_metric = 'auc',
                      outcome = outcome, n = 2)
```
Verified by debugger that:
- `perf_metric` is correctly set as a string when called from `model_feature_selection` for random forest.  The grid search only produces 1 result for lasso since our lasso is implemented with grid search for a unified framework with random forest.
- `selection_metric`/`perf_metric` is correctly used by h2o when is `pr_auc` and `auc`.  Other metrics that can be used are found in the model evaluation metrics list.

The following evaluation is shown for completeness for the individual model:

```{r}
rf_test_ind <- rf_model(outcome, 
                     training_frame = training_df,
                     validation_frame = validation_df,
                     nfolds = 5,
                     hyper_params = rf_params,
                     perf_metric = 'mean_per_class_error',
                     model_seed = 42)
```
Verified by debugger that:
- `perf_metric` correctly used at the model level
- `perf_metric` correctly used to choose best grid search model
- best grid search model correctly chosen.

## Using grid parallelization
You'll need to run 10, 30, 40, and 50 before attempting to run this code.

In the commit following 661a3f7d4 (again, I haven't committed yet), I try out adding the keyword `parallelism=0` parameter to the grid search.  The documentation [here](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html) states that this allows h2o to try to parallelize as much as possible. We'll see how this works on ACCRE, because the minute h2o uses up too much memory, the session will be terminated.  However, h2o is also memory bounded during the `init()` call, so we might just need to set that value higher.  Whatever the case, we'll see!

```{r}
filebase = '/scratch/p_gaydosh_lab'

## load waves and join them
wave_data <- load_waves(1:5, filebase=filebase)
full_dataset <- get_working_dataset_full(wave_data, join_type = 'full')

## set outcome variable of interest
outcome = 'h5mn8'

## get the aids that you want
inner_aids <- get_inner(list(wave_data[[1]], wave_data[[3]], wave_data[[4]], wave_data[[5]]))

## use the features and ids that you want to select out what you want
working_ds <- full_dataset %>%
  remove_subjects_not_in_wave1(filebase=filebase) %>%
  filter(aid %in% inner_aids) %>%
  dplyr::select(all_of(c('aid', emotional_despair_predictors, cognitive_despair_predictors, outcome))) %>%
  mutate_all(as.factor) %>%
  drop_na(outcome)

working_ds %>% glimpse()
```


 And then we're going to see if I notice any "eyeballed" differences in speed.  H2o's documentation does use a caveat word "small datasets" to signal that potentially, large datasets might not benefit from this operation.  We'll see.

I'm just going to borrow the following code to split right quick...

```{r, message=FALSE, warning=FALSE}

## split the data into relevant proportions desired
data_splits <- working_ds %>%
  split_data(strat_var = outcome, ratios=c(0.7, 0.2, 0.1))

# assemble list
training_df <- data_splits$train
validation_df <- data_splits$valid
testing_df <- data_splits$test
```

```{r}
h2o.init()
```


I'm going to run this code with and without the `parallelism = 0`.  Results will be reported below.
```{r feature selection rf, include=FALSE}

h2o.removeAll()

mt_list = seq(-1, 20, by=10)
mr_list = seq(5, 20, by=5)
bc_list = c(TRUE, FALSE)

# Spans of hyper parameters for random forest
rf_params <- list(max_depth = 50,
                  ntrees = 150,
                  mtries = mt_list,
                  min_rows = mr_list,
                  balance_classes = bc_list,
                  stopping_metric = 'AUCPR',
                  categorical_encoding = 'one_hot_explicit')

# rf_params <- list(max_depth = c(20, 50),
#                   balance_classes = TRUE,
#                   categorical_encoding= 'one_hot_explicit')

# define number of bootstraps
n_boot = 5

tic()
suicide_rf <- model_feature_selection("RF", training_frame = training_df,
                      validation_frame = validation_df,
                      hyper_params = rf_params,
                      outcome = outcome, n = n_boot, seed=2323)
toc()

```
Without parallelism enabled, it took 360.506 seconds to run 5 bootstraps.  Note that this is 5x3x3x2 models or grid searches with these hyperparameter settings.  This is essentially 4 seconds per model, which doesn't even sound that bad.  Let's see with parallelism...

Sometimes, the models are able to build, and the last time it took around 110 seconds.  However, this does not work reliably and sometimes the model builds fail due to race conditions.  So, as of 08/17/2020 and H2o version 3.30.1.1, this is not working - I receive NullPointerException errors and several grid models fail to build.  This is a known high priority issue documented in [H2o's JIRA issue tracker](https://0xdata.atlassian.net/browse/PUBDEV-7281).  I'm leaving the parameter in, set to default.  The current JIRA indicates that this should be fixed in 3.30.1.2, so we'll see if that update comes quickly and try it again then.

# Model seeds
Now, I'm going to evaluate the usage of model seeds to save all of the seeds that are randomly generated by h2o.

```{r}
h2o.init()
```

Let's just casually generate a model usings mtcars.  We'll predict the number of cylinders.  Then, we'll just look at the model parameters to see what is there.
```{r}
outcome <- 'cyl'
preds <- setdiff(names(mtcars), outcome)
demo_model <- h2o.glm(x = preds,
                      y= outcome,
                      training_frame = as.h2o(mtcars), 
                      alpha=1,
                      lambda_search=TRUE,
                      seed = -1)

# get random seed
demo_model@parameters$seed
```
The seed is saved in `demo_model@parameters$seed`.  The first time I ran this, the seed was "-3821245945289829439".  I ran it again and the seed was: "8472793909506248086".  This is good, and particularly good that we can get the seed, so now we can regenerate the model as desired.  Now, I'm going to quickly see how we can set h2o to generate a random seed.

It looks like if you pass in a `-1`, the seed is randomly generated.  This is good news if we want randomly generated model parameters.  This will also cause randomness in the k-folds generated.

#Aggregating model seeds

Now, I'll look into how we can aggregate all of the model seeds

```{r}
#Generate a list of models using the code above
t_mdl_list = list(demo_model, demo_model)

#use map to get the results
res <- t_mdl_list %>%
  map(function(x) x@parameters$seed)

res

```
This works.  Here, I'll test the function I've written `get_model_seeds` in 40 so that we can write them out to a csv file and save it if necessary.

```{r}
get_model_seeds(t_mdl_list)
```
This works correctly.

# Investigate kfold behavior
Now, we'll quickly see the behavior of k-fold cross validation based on setting the n-folds parameter vs setting the fold_column parameter and what happens when they're both present.

Note several considerations here:
- `nfolds` is either a boolean or a number.  It is not `NULL` to work.
- The fold assignment must be in the training frame, but it shouldn't be in the x or y arguments to the model joining.

```{r}

#mtcars has 32 rows.  Let's create a 2-fold split of equal size
fold_assigns = c(rep(1, 16), rep(2,16))

#let's go ahead and concatenate that onto mtcars
mtcars_fa <- cbind(mtcars, fold_assigns)

outcome <- 'cyl'
preds <- setdiff(names(mtcars_fa), c(outcome, 'fold_assigns'))

demo_model <- h2o.glm(x = preds,
                      y= outcome,
                      training_frame = as.h2o(mtcars_fa), 
                      nfolds = FALSE,
                      fold_column = 'fold_assigns',
                      keep_cross_validation_fold_assignment = TRUE,
                      alpha=1,
                      lambda_search=TRUE,
                      seed = -1)

head(h2o.cross_validation_fold_assignment(demo_model))

```

The above functionality results in the desired behavior of setting the folds, although it shows incorrectly, the fold assignments are correct.

Let's see what happens if we go ahead and set kfolds with a `NULL` setting for fold_column:

```{r}


#set null for fold column name to mimic what will happen for us
fold_column = NULL

outcome <- 'cyl'
preds <- setdiff(names(mtcars), c(outcome, fold_column))

demo_model <- h2o.glm(x = preds,
                      y= outcome,
                      training_frame = as.h2o(mtcars), 
                      nfolds = 5,
                      fold_column = fold_column,
                      keep_cross_validation_fold_assignment = TRUE,
                      alpha=1,
                      lambda_search=TRUE,
                      seed = -1)

head(h2o.cross_validation_fold_assignment(demo_model))

```
This also appears to work correctly.

Let's lastly just see what happens when they're both set:

```{r}

#mtcars has 32 rows.  Let's create a 2-fold split of equal size
fold_assigns = c(rep(1, 16), rep(2,16))

#let's go ahead and concatenate that onto mtcars
mtcars_fa <- cbind(mtcars, fold_assigns)

outcome <- 'cyl'
preds <- setdiff(names(mtcars_fa), c(outcome, 'fold_assigns'))

demo_model <- h2o.glm(x = preds,
                      y= outcome,
                      training_frame = as.h2o(mtcars_fa), 
                      nfolds = 5,
                      fold_column = 'fold_assigns',
                      keep_cross_validation_fold_assignment = TRUE,
                      alpha=1,
                      lambda_search=TRUE,
                      seed = -1)

head(h2o.cross_validation_fold_assignment(demo_model))

```
This correctly throws an error!  Yay!