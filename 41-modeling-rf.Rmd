---
title: "41-modeling-RF"
output: html_notebook
---

The purpose of this document is to build bootstrap training data n number of times and train n different random forest models and extract 
#Steps Taken

1. Splitting the data. [x]
2. Bootstrapping the training set n times (n = 1000). [x]
3. Splitting the bootstrap set into a training a validation set. [x]
4. Creating n different models. [x]
- finding the best tuning parameters for each model using grid search (Tuning parameters: ntrees and max_depth).[x]
- creating a final model (using k-fold cross validation) that uses the best tuning parameters from the previously conducted grid search.[x]
- fitting the model onto the validation set created earlier.[x]
5. Saving all models and the performance metrics into a list. [x]
6. Evaluation of all model parameters and the stability of those parameters.  

```{r load libraries}
library(data.table)
library(mlr)
library(splitstackshape) #stratified function
library(boot)
library(tidyverse)
library(rsample)
```


```{r random forest model function}
# This function gets the best features for the random fores by bootstrapping it n number of times
# Each bootstrapped model is then used to create 
rf_feature_selection <- function(training_df, 
                                 n = 2, 
                                 k = 5, 
                                 max_depths_tune = c(100,150,200), 
                                 ntree_tune = c(5,10,25), 
                                 outcome, 
                                 top_n = 20){
  # training_df : training data frame sent in after initial 70-30 split
  # n : n number of bootstrapped
  # k : k folds in cross-validation
  # max_depths_tune : tuning parameters to test for the max depth
  # ntree_tune : tuning parameters to test for number of trees in the random forest
  # outcome : name of the outcome variable
  # top_n : top n important variables to be extracted from the models generated
  # return : a list of three lists: 
  #          list 1 - all models created
  #          list 2 - all model performances
  #          list 3 - all dataframes of variable importances with the top_n variables in it
  
  
  # Helper function to create bootstrapped models
  boot_model <- function(train, outcome = outcome, max_depths_tune = max_depths_tune, ntrees_tune = ntree_tune ){  
    # train : bootstrapped training frame sent in to be created a model with
    
    # create lists to save models and results
    model_list <- list() 
    results_list <- list()
    
    # Step 3
    # Using splitstackshape::stratified to create a stratified split on the outcome variable - 90-10 split
    straps_splits <- stratified(as.data.frame(train), outcome, 0.9, bothSets = T) 
    
    # pull out training and validation frames
    training_frame <- as.h2o(straps_splits[[1]])
    validation_frame <- as.h2o(straps_splits[[2]])
    
    # Step 4
    # Create all combinations of tuning parameters
    params <- expand.grid(max_depth = max_depths_tune, ntrees = ntree_tune)
    model_tune_list <- rep(NA, nrow(params))
    
    #initialize function parameters for h2o.randomForest
    outputs <- outcome
    inputs <- setdiff(predictor_list, "aid")
    
    #Training models on all combinations of parameters
    for(i in 1:nrow(params)){
      rf_i <- h2o.randomForest(
        training_frame = training_frame,
        x= inputs,
        y= outputs, 
        model_id = "rf_covType_v1",
        max_depth = params[i,][[1]],
        ntrees = params[i,][[2]], 
        stopping_rounds = 2,
        nfolds = 5,
        fold_assignment = "Stratified",
        score_each_iteration = T,
        seed = 42
      )
      
      #Test on validation frame
      result <- h2o.performance(rf_i, as.h2o(validation_frame))
      
      #Pulling model metric (AUC)
      model_tune_list[i] <- result@metrics$AUC
    }
    
    # Pull the index of the tuning parameter which produced the highest AUC
    best_index <- which(model_tune_list==max(model_tune_list))[1]
    
    #Train final model based on best tuning parameters
    rf_i <- h2o.randomForest(
      training_frame = training_frame,
      x=inputs,
      y=outputs, 
      model_id = "rf_covType_v1",
      ntrees = params[best_index,2], 
      max_depth = params[best_index,1],
      stopping_rounds = 2,
      nfolds = k,
      fold_assignment = "Stratified",
      score_each_iteration = T,
      seed = 42
    )
    
    return(list(rf_i, h2o.performance(rf_i, validation_frame)))
  }
  
  #create bootstraps list
  straps <- training_df %>%bootstraps(times = n, strata = outcome)
  
  #lists to store results and models that are created on each bootstrap
  combined_models <- list()
  combined_performances <- list()
  variable_imps <- list()
  
  
  # Train the model on each of the bootstraps and get the performance 
  for(i in 1:length(straps$splits)){
    results <- as.data.frame(straps$splits[[i]])%>%
      boot_model(outcome = outcome, max_depths_tune = max_depths_tune, ntrees_tune = ntree_tune)
    
    combined_models[[i]] <- results[[1]]
    combined_performances[[i]] <- results[[2]]
    variable_imps[[i]] <- get_top_n_names(results[[1]], 20)
    
  }
  
  
  return(list(combined_models, combined_performances, variable_imps))
  
}

```




