---
title: "41-modeling-RF"
output: html_notebook
---

The purpose of this document is to build a randomforest that models the outcomes: suicidal ideation, marijuana usage and alcohol abuse. 

#Steps Taken

1. Splitting the data. [x]
2. Bootstrapping the training set n times (n = 1000). [x]
3. Splitting the bootstrap set into a training a validation set. [x]
4. Creating n different models. [x]
- finding the best tuning parameters for each model using grid search (Tuning parameters: ntrees and max_depth).[x]
- creating a final model (using k-fold cross validation) that uses the best tuning parameters from the previously conducted grid search.[x]
- fitting the model onto the validation set created earlier.[x]
5. Saving all models and the performance metrics into a list. [x]
6. Evaluation of all model parameters and the stability of those parameters.  



```{r load libraries}
library(data.table)
library(mlr)
library(splitstackshape) #stratified function
library(boot)
library(tidyverse)
library(rsample)
```


```{r random forest model function}
# Note: The steps are not in order because the function needed to be created first.
rf_model <- function(#training_frame,         
  #validation_frameï¼Œ
  train,          # training set or bootstrap
  k = 5,          # ks of k fold
  max_depths_tune = c(100,150,200), # vector of max_depths to test on
  ntree_tune = c(5,10,25), # vector of ntrees to test on
  outcome)
  {     
  
  
    # create lists to save models and results
    model_list <- list() 
    results_list <- list()
    
  
    # Step 3
    # Using splitstackshape::stratified to create a stratified split on the outcome variable - 90-10 split
    straps_splits <- stratified(as.data.frame(train), outcome, 0.9, bothSets = T) 
    
    # pull out training and validation frames
    training_frame <- as.h2o(straps_splits[[1]])
    validation_frame <- as.h2o(straps_splits[[2]])
    
    # Step 4
    # Create all combinations of tuning parameters
    params <- expand.grid(max_depth = max_depths_tune, ntrees = ntree_tune)
    model_tune_list <- rep(NA, nrow(params))
    
    #initialize function parameters for h2o.randomForest
    outputs <- outcome
    inputs <- setdiff(predictor_list, "aid")
    
    #Training models on all combinations of parameters
    for(i in 1:nrow(params)){
      rf_i <- h2o.randomForest(
        training_frame = training_frame,
        x= inputs,
        y= outputs, 
        model_id = "rf_covType_v1",
        max_depth = params[i,][[1]],
        ntrees = params[i,][[2]], 
        stopping_rounds = 2,
        nfolds = 5,
        fold_assignment = "Stratified",
        score_each_iteration = T,
        seed = 42
      )
      
      #Test on validation frame
      result <- h2o.performance(rf_i, as.h2o(validation_frame))
      
      #Pulling model metric (AUC)
      model_tune_list[i] <- result@metrics$AUC
    }
    
    # Pull the index of the tuning parameter which produced the highest AUC
    best_index <- which(model_tune_list==max(model_tune_list))[1]
    
    #Train final model based on best tuning parameters
    rf_i <- h2o.randomForest(
      training_frame = training_frame,
      x=inputs,
      y=outputs, 
      model_id = "rf_covType_v1",
      ntrees = params[best_index,2], 
      max_depth = params[best_index,1],
      stopping_rounds = 2,
      nfolds = k,
      fold_assignment = "Stratified",
      score_each_iteration = T,
      seed = 42
    )
    
    
    return(list(rf_i, h2o.performance(rf_i, validation_frame)))
}

```
