---
title: "51-modeling-pass1"
output: html_document
---

The purpose of this document is to create the models used for the exploration in the file 60 series.

```{r load libraries}
# these are the libraries needed to run the code below
library(data.table)
#library(mlr)
library(splitstackshape) #stratified function
library(boot)
library(tidyverse)
library(rsample)
```


```{r convenience functions}
# This function performs a stratified split
strat_split <- function(df, strat_var, split_prop, train_out = T){
  # df: input dataframe
  # strat_var: variable to stratify on
  # train_prop: proption to split for train/testing
  # train_out: set to TRUE for training frame, set to FALSE for validation frame
  # returns: training or testing tibble
  
  # create stratified split
  splits <- stratified(df, strat_var, split_prop, bothSets = T)
  train <- splits[[1]]
  test <- splits[[2]]
  
  # return tibble of training set if train_out == True
  if(train_out == T){out <- train}
  else{out<-test}
  return(out)
}

# Get total value based on placement of variable importance
get_total_placement <- function(list){
  # list: a list of data frames returned from the get_top_n_names function
  # returns: a data frame grouped by all variables with sum total of placement
  
  out <- list %>%
    purrr::reduce(rbind) %>% 
    group_by(variable) %>% 
    summarise(total = sum(placement))
  
  # return tibble
  return(out)
}

```

### Random forest model implementation to predict suicidal ideation
Testing the functions by training a random forest model 
```{r create splits for training and testing on suicide_df}
# Get training frame
training_df <- suicide_df %>% 
  strat_split(strat_var = c("h5mn8"),split_prop = 0.7, train_out = TRUE)

# Get testing frame
testing_df <- suicide_df %>% 
  strat_split(c("h5mn8"),0.7, FALSE)



# Using splitstackshape::stratified to create a stratified split on the outcome variable - 90-10 split
straps_splits <- stratified(as.data.frame(training_df), suicide_w5, 0.9, bothSets = T) 

# pull out training and validation frames
training_frame <- as.h2o(straps_splits[[1]])
validation_frame <- as.h2o(straps_splits[[2]])


# set hyper parameters for random forest
rf_params <- list(max_depth = c(100,150,200), 
                   ntrees = c(5,10,25))
```


```{r test suicide model}
# suicide_w5 was created earlier - it is the suicide indicator at wave 5

suicide_out <- training_df %>% 
  model_feature_selection("RF",training_frame = training_frame,
                      validation_frame = validation_frame,
                      hyper_params = rf_params,
                      outcome = suicide_w5, n = 5)
```



```{r explore variable importance for suicide, fig.width=15,fig.height=6}
# This will give you a data frame of top predictors

top_preds_outcome <- suicide_out[[3]] %>%
  get_total_placement() %>% 
  mutate(avg_place = total /n,
         top_20 = if_else(avg_place <= 20, "Top 20", "Not Top 20"))

# Box plot after feature selection
select_RFvar_all <- suicide_out[[3]]%>%
    purrr::reduce(rbind)

select_RFvar_all%>%
  group_by(variable)%>% 
  arrange(desc(placement))%>%
  ggplot(aes(x = variable, y = placement)) +
  geom_boxplot()

# Plot top predictors
top_preds_outcome %>% 
  arrange(desc(total)) %>%
  ggplot(aes(x = reorder(variable, -avg_place), y = avg_place)) +
  geom_col(aes(fill = top_20)) +
  geom_hline(yintercept = 20, color = "yellow") +
  xlab("Predictor Variable") + 
  ylab("Average Importance") +
  ggtitle("Variable Importance based on Bootstrap") +
  coord_flip() +
  theme_dark()

top_preds_outcome %>% 
  arrange(avg_place)
```

```{r create new df based on top features}
#subsetting the suicide df based on the top 20 features
final_rf_df <- suicide_df %>% 
  get_outcomes_df(suicide_w5, top_preds_outcome$variable)

```


```{r hyperparamter tuning for different spans of hyperparameters}

outcome <- suicide_w5
max_depths_tune <- seq(1,30,2)
ntrees_tune <- seq(50, 300, 25)
min_rows_tune <- seq(10,500,40)

# 80-20 split to create a validation frame
straps_splits <- stratified(as.data.frame(final_rf_df), outcome, 0.8, bothSets = T) 

# pull out training and validation frames
training_frame <- as.h2o(straps_splits[[1]])
validation_frame <- as.h2o(straps_splits[[2]])

# Create all combinations of tuning parameters
params <- expand.grid(max_depth = max_depths_tune, ntrees = ntrees_tune, min_rows = min_rows_tune)
model_tune_list <- rep(NA, nrow(params))

#initialize function parameters for h2o.randomForest
outputs <- outcome
inputs <- setdiff(names(final_rf_df), outcome)

#Training models on all combinations of parameters
for(i in 1:nrow(params)){
  rf_i <- h2o.randomForest(
    training_frame = training_frame,
    x= inputs,
    y= outputs, 
    model_id = "rf_covType_v1",
    max_depth = params[i,][[1]],
    ntrees = params[i,][[2]],
    min_rows = params[i,][[3]],
    stopping_rounds = 2,
    nfolds = 5,
    fold_assignment = "Stratified",
    score_each_iteration = T,
    seed = 42
  )
  
  #Test on validation frame
  result <- h2o.performance(rf_i, as.h2o(validation_frame))
  
  #Pulling model metric (AUC)
  model_tune_list[i] <- result@metrics$AUC
}

# Pull the index of the tuning parameter which produced the highest AUC
best_index <- which(model_tune_list==max(model_tune_list))[1]

```

```{r}
rf_final_model <- h2o.randomForest(
  training_frame = as.h2o(training_df),
  x= inputs,
  y= outputs,
  model_id = "rf_covType_v1",
  max_depth = params[best_index,][[1]],
  ntrees = params[best_index,][[2]], 
  min_rows = params[best_index,][[3]],
  stopping_rounds = 2,
  nfolds = 5,
  fold_assignment = "Stratified",
  score_each_iteration = T,
  seed = 42
)
```


### Lasso model implementation to predict suicidal ideation
Testing the functions by training a lasso model 

```{r}
# set hyper parameters for random forest
lasso_params <- list(alpha = c(1))
```

```{r test suicide model}
# suicide_w5 was created earlier - it is the suicide indicator at wave 5

suicide_out_lasso <- training_df %>% 
  model_feature_selection("Lasso",training_frame = training_frame,
                      validation_frame = validation_frame,
                      hyper_params = lasso_params,
                      outcome = suicide_w5, 
                      n = 2)
```



```{r explore variable importance for suicide, fig.height=6, fig.width=20}
# This will give you a data frame of top predictors

top_preds_outcome_lasso <- suicide_out_lasso[[3]] %>%
  get_total_placement() %>% 
  mutate(avg_place = total /n,
         top_20 = if_else(avg_place <= 20, "Top 20", "Not Top 20"))


# Box plot after feature selection
select_LSvar_all <- suicide_out_lasso[[3]]%>%
    purrr::reduce(rbind)

# for (i in 1:length(select_LSvar_all$variable)){
#   select_LSvar_all$variable[i] <- unlist(strsplit(select_LSvar_all$variable[i],
#                                                          split = '.', fixed = TRUE))[1]
# }

select_LSvar_all%>%
  group_by(variable)%>% 
  arrange(desc(placement))%>%
  ggplot(aes(x = variable, y = placement)) +
  geom_boxplot()



# Plot top predictors
# for (i in 1:length(top_preds_outcome_lasso$variable)){
#   top_preds_outcome_lasso$variable[i] <- unlist(strsplit(top_preds_outcome_lasso$variable[i],
#                                                          split = '.', fixed = TRUE))[1]
# }


top_preds_outcome_lasso %>% 
  arrange(desc(total)) %>%
  ggplot(aes(x = reorder(variable, -avg_place), y = avg_place)) +
  geom_col(aes(fill = top_20)) +
  geom_hline(yintercept = 20, color = "yellow") +
  xlab("Predictor Variable") + 
  ylab("Average Importance") +
  ggtitle("Variable Importance based on Bootstrap") +
  coord_flip() +
  theme_dark()



top_preds_outcome_lasso %>% 
  arrange(avg_place)
```

```{r create new df based on top features}
#subsetting the suicide df based on the top 20 features
top_preds_outcome_lasso$variable

for (i in 1:length(top_preds_outcome_lasso$variable)){
  top_preds_outcome_lasso$variable[i] <- unlist(strsplit(top_preds_outcome_lasso$variable[i],
                                                         split = '.', fixed = TRUE))[1]
}

top_lasso_var <- unique(top_preds_outcome_lasso$variable)

final_lasso_df <- suicide_df %>% 
  get_outcomes_df(suicide_w5, top_lasso_var)

```


```{r hyperparamter tuning for different spans of hyperparameters}

outcome <- suicide_w5
lambda_tune = seq(0,0.02,0.002)

# 80-20 split to create a validation frame
straps_splits_lasso <- stratified(as.data.frame(final_lasso_df), outcome, 0.8, bothSets = T)

# pull out training and validation frames
training_frame_lasso <- as.h2o(straps_splits_lasso[[1]])
validation_frame_lasso <- as.h2o(straps_splits_lasso[[2]])

# Create all combinations of tuning parameters
params_lasso <- expand.grid(lambda = lambda_tune)
model_tune_list_lasso <- rep(NA, nrow(params_lasso))

#initialize function parameters for h2o.randomForest
outputs <- outcome
inputs <- setdiff(names(final_lasso_df), outcome)

#Training models on all combinations of parameters
for(i in 1:nrow(params_lasso)){
lasso_i <- h2o.glm(
      training_frame = training_frame_lasso,
      x=inputs,
      y=outputs,
      model_id = NULL,
      lambda = params_lasso[i,][[1]],
      family = "binomial",
      alpha = 1, # for Lasso
      nfolds = 5,
      fold_assignment = "Stratified",
      score_each_iteration = T,
      seed = 42
    )
  
  #Test on validation frame
  result_lasso <- h2o.performance(lasso_i, as.h2o(validation_frame_lasso))
  
  #Pulling model metric (AUC)
  model_tune_list_lasso[i] <- result_lasso@metrics$AUC
}

# Pull the index of the tuning parameter which produced the highest AUC
best_index_lasso <- which(model_tune_list_lasso==max(model_tune_list_lasso))[1]

```

```{r}
lasso_final_model <- h2o.glm(
  training_frame = as.h2o(training_df),
  x= inputs,
  y= outputs,
  lambda = params_lasso[best_index_lasso,1],
  family = "binomial",
  alpha = 1,
  nfolds = 5,
  fold_assignment = "Stratified",
  score_each_iteration = T,
  seed = 42
)
```

