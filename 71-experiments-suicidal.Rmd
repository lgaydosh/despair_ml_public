---
title: "71-experiments-suicidal"
output: 
  html_document:
    code_folding: 'show'
    toc: true
    toc_depth: 3
---

```{r}
source("function_import.R")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Experiments Template

**Purpose.** The purpose of this document is to serve as a template for the future experiments that will be conducted using the code in this repo.  To generate the desired document, you may erase this and the above lines so that the experiment name directly follows the document title.

# Experiment name

**Purpose.** In this work, we will explore the relation between identified measures of despair of interest (e.g., personality measures of self-consciousness, individual and composite item scores from the CES-D assessment) and descriptors of diseases of despair.  We will achieve this goal through modeling the outcomes based on the included predictors, and robustly assess the importance of the included features in predicting the outcomes via bootstrapping.  We will use two well-known machine learning models, random forests and LASSO, which are both frequently used to measure the relative importance of the predictors included in the models.  Lastly, we'll generate trained and tuned models using this reduced feature set which can be used by others wish to predict the identified outcomes.

**Subject inclusion.** For this investigation, we will omit the entirety of Wave 2.  This is commonly done in analyses of AddHealth data due the design of the original study.  Otherwise, our dataset will include only subjects who have predictor and outcome data in _all_ of the waves.

**Outcome variables.** Specifically, we will model two binary outcomes variables: _suicidal ideation_ and _marijuana use_, assessed at Wave 5.  

**Predictor variables.** The predictors for these models are hand-picked, and based on previous work, relevance, and subject matter expertise. The set of predictors and the set of outcomes are disjoint.  Predictors from Waves 1-4 (excluding Wave 2, see above) are included, and will be detailed in the following analysis.

```{r load libraries}
# Use pacman, which forces an install if the library isn't present on the running machine
if (!require("pacman")) install.packages("pacman")

pacman::p_load(SASxport, haven, dplyr, tidyr, janitor, naniar, ggplot2, Hmisc, purrr, scales, reshape, HH, tibble, likert, reshape2,forcats,splitstackshape,rsample, h2o)

```

```{r h2o initialization}
h2o.init(nthreads = -1,
         max_mem_size = "8G")  
```

# Dataset generation
Here, we comment on the general size and shape of the data and provide justification if necessary.
```{r load data}
# Loading of joined data parameter list here (e.g., join type, predictor list, outcomes list)
join_type <- 'inner'
preds_list <- c(anxiety_list, diagnosed_anxiety_list, optimism_list, depression_list, diagnosed_depression_list) 
out_list <- c('h5mn8')

# Load joined data based on desired manner of join, predictor list, and outcome variable
wave_data <- load_waves(1:5, filebase = 'Y:') # change filebase based on where you are running this from
full_dataset <- get_working_dataset_full(feature_list = preds_list, wave_data) 


# Report about the characteristics of the subjects left out of the join

# Validate the generated dataset using asserts


# I will use suicide_df for now
```

```{r}
wave_data <- load_waves(1:5, filebase = 'Y:')
full_dataset <- get_working_dataset_full(wave_data, join_type = 'full')

## set outcome variable of interest
outcome = 'h5mn8'

## get the aids that you want
inner_aids <- get_inner(list(wave_data[[1]], wave_data[[3]], wave_data[[4]], wave_data[[5]]))

## use the features and ids that you want to select out what you want
working_ds <- full_dataset %>%
                filter(aid %in% inner_aids) %>%
                select(aid, predictor_list, outcome)
```

```{r}
aid_lists <- wave_data %>% map(select(aid))
```





# Data exploration and visualization
Here, we comment about the general characteristics of the data based on the provided visualizations.  We comment on missingness of data, any strange or unusual behavior (e.g., strong imbalances), and any correlation that sticks out.
```{r eda}
# Visualize distributions of variables of interest

# Visualize missingness

# Visualize correlation among predictors
```

# Robust feature selection {.tabset .tabset-fade .tabset-pills}
- Here, we will justify the feature selection based on performance metrics; we assert that because of good model performance, we can expect that the features selected are reasonable because the models fit the data well (without being overtrained)
- Here, the subject matter experts will comment on the selected features and their general applicability to the outcomes of interest.
- Lastly, we will comment on differences in results between RF and LASSO

## RF model
Here, we will comment on the features selected by the random forest, its training/validation performance, and the parameters resulting in the best model.
```{r feature selection rf}
# Function parameters
# Get training frame
training_df <- suicide_df %>% 
  strat_split(strat_var = c("h5mn8"),split_prop = 0.7, train_out = TRUE)

# Get testing frame
testing_df <- suicide_df %>% 
  strat_split(c("h5mn8"),0.7, FALSE)


# Using splitstackshape::stratified to create a stratified split on the outcome variable - 90-10 split
straps_splits <- stratified(as.data.frame(training_df), suicide_w5, 0.9, bothSets = T) 

# pull out training and validation frames
training_frame <- straps_splits[[1]]
validation_frame <- straps_splits[[2]]


# Spans of hyper parameters for random forest
#   ntrees  : 50-150
#   max_depths : 15-30
#   mtries : -1,-2,3,4,5
#   min_rows : 25 - 60
rf_params <- list(max_depth = c(15,20,25,30), 
                  ntrees = c(50,75,100,125,150),
                  mtries = -1,
                  min_rows = c(25,40,60))


suicide_out <- model_feature_selection("RF",training_frame = training_frame,
                      validation_frame = validation_frame,
                      hyper_params = rf_params,
                      outcome = suicide_w5, n = 2)

top_preds_outcome <- suicide_out[[3]] %>%
  get_total_placement() %>% 
  mutate(avg_place = total /n,
         top_20 = if_else(avg_place <= 20, "Top 20", "Not Top 20"))

# Box plot after feature selection
select_RFvar_all <- suicide_out[[3]]%>%
    purrr::reduce(rbind)

select_RFvar_all

select_RFvar_all%>%
  group_by(variable)%>%
  ggplot() +
  geom_boxplot(aes(x = fct_reorder(variable, -placement), y = placement))

# Plot top predictors
top_preds_outcome %>% 
  arrange(desc(total)) %>%
  ggplot(aes(x = reorder(variable, -avg_place), y = avg_place)) +
  geom_col(aes(fill = top_20)) +
  geom_hline(yintercept = 20, color = "yellow") +
  xlab("Predictor Variable") + 
  ylab("Average Importance") +
  ggtitle("Variable Importance based on Bootstrap") +
  coord_flip() +
  theme_dark()

top_preds_outcome %>% 
  arrange(avg_place)

# Call modeling function using function parameters and show visualization of results.  Recommend the number of features that should be used.  Report average performance metrics.


results <- testing_df %>% 
  get_metrics(h2o_model = rf_final_model)



```

## LASSO model
Here, we will comment on the features selected by LASSO, its training/validation performance, and the parameter resulting in the best model.
```{r feature selection lasso}
# Function parameters

# Call modeling function using function parameters and show visualization of results.  Recommend the number of features that should be used.  Report performance metric stats.

```

## Model comparison
Here, we will comment in depth about the differences between these selected features
```{r feature selection comparison}
# Comparison of performance metrics

# Comparison of top_n features
```

# Generation of final model {.tabset .tabset-fade .tabset-pills}
Here, we will discuss the performance of the final models, including performance metrics.  We will comment on the most important features that it selected.
The subject matter experts will comment on the relevance of these features to the outcomes of interest.

## RF model
Here, we will comment on the behavior and performance of the final rf model.
```{r final model evaluation rf}
# Function parameters

# Call final modeling function and show visualization of results.  Report performance metric stats.
```

## LASSO model
Here, we'll comment on the behavior and performance of the final lasso model.
```{r final model evaluation lasso}
# Function parameters

# Call final modeling function and show visualization of results.  Report performance metric stats.
```

## Final model comparison
Here, we'll describe the differences between the final models.
```{r final model comparison}
# Comparison of performance metrics

# Comparison of top_n features
```

# Outcome variable discussion
Here, the subject matter experts will comment on the the differences in the features obtained between the studied outcomes variables and discuss the discrepancies and/or cohesion.
```{r outcome variable comparison}
# Show differences in features obtained

```

