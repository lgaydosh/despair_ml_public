---
title: "99-ROC-graphs"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r source files, include=FALSE}
source("function_import.R")
```

```{r load libraries, include=FALSE}
# Use pacman, which forces an install if the library isn't present on the running machine
if (!require("pacman")) install.packages("pacman")
#pacman::p_install(plotly)
pacman::p_load(tidyverse, h2o, furrr)

```

```{r initializations, include=FALSE}
port_no <- start_h2o()
h2o.no_progress()
future::plan(multicore)
```


```{r}
library(ggplot2)
library(ROCR)
```


```{r}
# get predictions
get_preds <- function(test_data, test_model, thresh_met = 'f1', thresh_val=NULL, outcome){
  
  if(is.null(thresh_val)){
    
    # get parameters for computing threshold of interest
    met_name = str_c('max ', thresh_met)
    model_metnames <- test_model@model$training_metrics@metrics$max_criteria_and_metric_scores$metric
    model_threshes <- test_model@model$training_metrics@metrics$max_criteria_and_metric_scores$threshold
    
    #Get the threshold of maximum metric of interest
    thresh_val <-model_threshes[which(model_metnames==met_name)]
    
  }
  
  #get actual column
  act <- test_data %>%
    dplyr::select(outcome) %>%
    dplyr::rename(actual = outcome)
  
  #get predictions - this returns predictions according to the training f1 score
  #releveling is done because yardstick wants to see the first level as the level of interest.
  preds <- h2o.predict(test_model, as.h2o(test_data)) %>% 
    as_tibble() %>%
    dplyr::select(-predict) %>%
    mutate(predict = as.factor(as.numeric(p1>thresh_val))) %>%
    bind_cols(act) %>%
    mutate(predict = fct_expand(predict, "0", "1"), predict = fct_relevel(predict, "1", "0")) %>% #make sure we always have the right levels available (e.g., if predict only predicts 0)
    mutate(actual = fct_expand(actual, "0", "1"), actual = fct_relevel(actual, "1", "0"))
  
  
  return(preds)

}

```





### For one case
```{r}
#AUC and ROC for validation set

#for ROC curve
preds_validation_rf = get_preds(validation_df, final_model_rf, outcome = outcome)
preds_validation_lasso = get_preds(validation_df, final_model_lasso, outcome = outcome)

#for AUC value
auc_validation_rf = preds_validation_rf %>% 
  roc_auc(truth=actual, p1) %>%
  pull(.estimate)

auc_validation_lasso = preds_validation_lasso %>% 
  roc_auc(truth=actual, p1) %>%
  pull(.estimate)

plot_validation_rf = autoplot(roc_curve(preds_validation_rf, truth=actual, p1), xlab = "False positive rate", ylab = "True positive rate", title = "Marks show times with censoring") + ggtitle("ROC for final RF model \non validation set") + labs(y="True positive rate (sensitivity)", x = "False positive rate (1 - specificity)") + annotate("text", label = paste("AUC = ",round(auc_validation_rf, 6)), x = 0.75, y = 0.25)


plot_validation_lasso = autoplot(roc_curve(preds_validation_lasso, truth=actual, p1), xlab = "False positive rate", ylab = "True positive rate", title = "Marks show times with censoring") + ggtitle("ROC for final LASSO model \non validation set") + labs(y="True positive rate (sensitivity)", x = "False positive rate (1 - specificity)") + annotate("text", label = paste("AUC = ",round(auc_validation_rf, 6)), x = 0.75, y = 0.25)




#AUC and ROC for testing set


#for ROC curve
preds_testing_rf = get_preds(testing_df, final_model_rf, outcome = outcome)
preds_testing_lasso = get_preds(testing_df, final_model_lasso, outcome = outcome)


#for AUC value
auc_testing_rf = preds_testing_rf %>% 
  roc_auc(truth=actual, p1) %>%
  pull(.estimate)

auc_testing_lasso = preds_testing_lasso %>% 
  roc_auc(truth=actual, p1) %>%
  pull(.estimate)


plot_testing_rf = autoplot(roc_curve(preds_testing_rf, truth=actual, p1), xlab = "False positive rate", ylab = "True positive rate", title = "Marks show times with censoring") + ggtitle("ROC for final RF model \non testing set") + labs(y="True positive rate (sensitivity)", x = "False positive rate (1 - specificity)") + 
 annotate("text", label = paste("AUC = ",round(auc_testing_rf, 6)), x = 0.75, y = 0.25)


plot_testing_lasso = autoplot(roc_curve(preds_testing_lasso, truth=actual, p1), xlab = "False positive rate", ylab = "True positive rate", title = "Marks show times with censoring") + ggtitle("ROC for final LASSO model \non testing set") + labs(y="True positive rate (sensitivity)", x = "False positive rate (1 - specificity)") + 
 annotate("text", label = paste("AUC = ",round(auc_testing_lasso, 6)), x = 0.75, y = 0.25)



#### Plots

grid.arrange(plot_validation_rf, plot_validation_lasso, ncol=2)

grid.arrange(plot_testing_rf, plot_testing_lasso, ncol=2)
```










###AUC and ROC for all outcomes

/results_full_h2o_2
i_drug
FALSE

/results_full_h2o_2
hv_drink
FALSE

/results_full_h2o_2_tsunk
p_drug
FALSE

/results_run_1108
prob_drink
FALSE

##### basic settings
```{r}
seed = 9384
filebase = '/scratch/p_gaydosh_dsi'
set.seed(seed)
fbase = '/scratch/p_gaydosh_dsi/DSI/'
met <- 'pr_auc'
```

```{r}
all_outcomes = c("i_drug", "h5mn8", "prob_drink")  #"p_drug"
binarize= FALSE
all_results_directory = c("/new_data_1", "/new_data_1",  "/results_run_0316_25") #"/results_3.17",
```

##### store good tasks for each outcome in a list
```{r}
no_tasks = vector('list', 3) #4
for (i in 1:length(all_results_directory)){
  
  results_dir <- str_c(fbase, all_outcomes[i], all_results_directory[i])
  good_tasks <- get_good_task_ids(results_dir)
  
  no_tasks[[i]] <- good_tasks
}
```



##### store full_dataset, validation_df, testing_df for each outcome in three lists
```{r}
full_dataset = vector('list', 3) #4
validation_df = vector('list', 3) #4
testing_df  = vector('list', 3) #4

## for loop doesn't work well if session has low memory and few cores
for (i in 1:length(all_outcomes)){

  df <- generate_datasets(all_outcomes[i], binarize=binarize, filebase=filebase, seed_val=seed)
  full <- df$full_dataset
  full_dataset[[i]] <- full

  vali <- df$validation_df
  validation_df[[i]] <- vali

  test <- df$testing_df
  testing_df[[i]] <- test

}

# run from 1 to 4
  # df <- generate_datasets(all_outcomes[4], binarize=binarize, filebase=filebase, seed_val=seed)
  # full <- df$full_dataset
  # full_dataset[[4]] <- full
  # 
  # vali <- df$validation_df
  # validation_df[[4]] <- vali
  # 
  # test <- df$testing_df
  # testing_df[[4]] <- test
```



##### store final_model_rf, final_rf_perm for each outcome in two lists
```{r}
final_model_rf = vector('list', 3) #4
final_rf_perm = vector('list', 3) #4

for (i in 1:length(all_outcomes)){
  
  results_dir <- str_c(fbase, all_outcomes[i], all_results_directory[i])
  
  final_rf_perf = NULL
  final_rf_perfs = load_from_csv(final_rf_perf, results_dir, no_tasks[[i]])
  final_rfmodel_ind <- which.max(dplyr::select(final_rf_perfs, met) %>% pull(met))
  final_model_rf_list <- load_best_model('final_rf_model', results_dir, no_tasks[[i]][final_rfmodel_ind])
  
  final_model_rf[[i]] <- final_model_rf_list
  
  
  
  final_rf_perm_plt = NULL
  final_rf_perm_plt = load_from_csv(final_rf_perm_plt, results_dir, best_ind=no_tasks[[i]][final_rfmodel_ind])
  final_rf_perm_list <- final_rf_perm_plt %>%
    get_permute_placement(metric_oi=met) %>%
    add_attribute_names('predictor', full_dataset[[i]]) %>%
    dplyr::select(predictor, everything())
  
  final_rf_perm[[i]] <- final_rf_perm_list
  
  
}
```




##### store final_model_lasso, final_lasso_perm for each outcome in two lists
```{r}
final_model_lasso = vector('list', 3) #4
final_lasso_perm = vector('list', 3) #4

for (i in 1:length(all_outcomes)){
  
  results_dir <- str_c(fbase, all_outcomes[i], all_results_directory[i])
  
  final_lasso_perf = NULL
  final_lasso_perfs = load_from_csv(final_lasso_perf, results_dir, no_tasks[[i]])
  final_lassomodel_ind <- which.max(dplyr::select(final_lasso_perfs, met) %>% pull(met))
  final_model_lasso_list <- load_best_model('final_lasso_model', results_dir, no_tasks[[i]][final_lassomodel_ind])
  
  final_model_lasso[[i]] <- final_model_lasso_list
  
  
  
  
  final_lasso_perm_plt = NULL
  final_lasso_perm_plt = load_from_csv(final_lasso_perm_plt, results_dir, best_ind=no_tasks[[i]][final_lassomodel_ind])
  final_lasso_perm_list <- final_lasso_perm_plt %>%
    get_permute_placement(metric_oi=met) %>%
    add_attribute_names('predictor', full_dataset[[i]]) %>%
    dplyr::select(predictor, everything())

  
  final_lasso_perm[[i]] <- final_lasso_perm_list
}
```



##### store predictions (for RF, LASSO on validation and testing set) with the get_preds function in seperate lists
```{r}
preds_validation_rf = vector('list', 3) #4
preds_validation_lasso = vector('list', 3)  #4
preds_testing_rf = vector('list', 3)  #4
preds_testing_lasso = vector('list', 3)  #4

for (i in 1:length(all_outcomes)){
  
  preds_validation_rf_list = get_preds(validation_df[[i]], final_model_rf[[i]], outcome = all_outcomes[[i]])
  preds_validation_rf[[i]] <- preds_validation_rf_list
  
  preds_validation_lasso_list = get_preds(validation_df[[i]], final_model_lasso[[i]], outcome = all_outcomes[[i]])
  preds_validation_lasso[[i]] <- preds_validation_lasso_list
  
  preds_testing_rf_list = get_preds(testing_df[[i]], final_model_rf[[i]], outcome = all_outcomes[[i]])
  preds_testing_rf[[i]] <- preds_testing_rf_list
  
  preds_testing_lasso_list = get_preds(testing_df[[i]], final_model_lasso[[i]], outcome = all_outcomes[[i]])
  preds_testing_lasso[[i]] <- preds_testing_lasso_list
}
```



##### store auc values (for RF, LASSO on validation and testing set) in seperate lists
```{r}
auc_validation_rf = vector('list', 3) #4
auc_validation_lasso = vector('list', 3) #4
auc_testing_rf = vector('list', 3) #4
auc_testing_lasso = vector('list', 3) #4

for (i in 1:length(all_outcomes)){
  
  auc_validation_rf_list = preds_validation_rf[[i]] %>% 
    roc_auc(truth=actual, p1) %>%
    pull(.estimate)
  auc_validation_rf[[i]] <- auc_validation_rf_list
  
  auc_validation_lasso_list = preds_validation_lasso[[i]] %>% 
    roc_auc(truth=actual, p1) %>%
    pull(.estimate)
  auc_validation_lasso[[i]] <- auc_validation_lasso_list
  
  auc_testing_rf_list = preds_testing_rf[[i]] %>% 
    roc_auc(truth=actual, p1) %>%
    pull(.estimate)
  auc_testing_rf[[i]] <- auc_testing_rf_list
  
  auc_testing_lasso_list = preds_testing_lasso[[i]] %>% 
    roc_auc(truth=actual, p1) %>%
    pull(.estimate)
  auc_testing_lasso[[i]] <- auc_testing_lasso_list
}
```

# join
```{r}
preds_validation_rf
```

```{r}
load("experimental_results.RData")
```

```{r}
pred
```

legend("bottomright", 
 legend=c(paste("Suic. idea. with AUC = ",round(auc_validation_rf[[2]], 3)),
          paste("Ill. drug with AUC = ",round(auc_validation_rf[[1]], 3)),
          # paste("Rx abuse with AUC = ",round(auc_validation_rf[[3]], 3)), 
          paste("Prob. drink with AUC = ",round(auc_validation_rf[[3]], 3)),
          paste("Rx abuse with AUC = ",round(auc_validation_rf[[3]], 3))),
          # paste("prob_drink with AUC = ",round(auc_validation_rf[[4]], 3))),
##### ROC graph for final RF model on validation set for all outcomes
```{r}

# validation and rf
pred2 <- prediction(preds_validation_rf[[1]]$p1, preds_validation_rf[[1]]$actual)
pred3 <- prediction(preds_validation_rf[[2]]$p1, preds_validation_rf[[2]]$actual)
pred4 <- prediction(preds_validation_rf[[3]]$p1, preds_validation_rf[[3]]$actual)
#pred4 <- prediction(preds_validation_rf[[4]]$p1, preds_validation_rf[[4]]$actual)
perf2 <- performance(pred2, "tpr", "fpr" )
perf3 <- performance(pred3, "tpr", "fpr")
perf4 <- performance(pred4, "tpr", "fpr" )
#perf4 <- performance(pred4, "tpr", "fpr")
plot(perf2, col = "red", main = "ROC for final RF model \non validation set for all outcomes") + abline(coef = c(0,1)) 
plot(perf3, add = TRUE, col = "blue")
plot(perf4, add = TRUE, col = "green")
plot(perf, add = TRUE, col = "yellow")
#plot(perf4, add = TRUE, col = "orange")

legend("bottomright", 
 legend=c(paste("Suic. idea. with AUC = ",round(auc_validation_rf[[2]], 3)),
          paste("Ill. drug with AUC = ",round(auc_validation_rf[[1]], 3)),
          # paste("Rx abuse with AUC = ",round(auc_validation_rf[[3]], 3)), 
          paste("Prob. drink with AUC = ",round(auc_validation_rf[[3]], 3)),
          paste("Rx abuse with AUC = 0.866")),
          # paste("prob_drink with AUC = ",round(auc_validation_rf[[4]], 3))),
 bty = "n",lwd=2, cex=0.8,y.intersp=1.0, col=c("blue","red","green", "yellow"), lty=c(1,1,1,1)) # lty=c(1,1,1,1) col=c("blue","red","green", "orange")
```



##### ROC graph for final LASSO model on validation set for all outcomes
```{r}
# validation and lasso
pred_lasso <- prediction(preds_validation_lasso[[1]]$p1, preds_validation_lasso[[1]]$actual)
pred2_lasso <- prediction(preds_validation_lasso[[2]]$p1, preds_validation_lasso[[2]]$actual)
pred3_lasso <- prediction(preds_validation_lasso[[3]]$p1, preds_validation_lasso[[3]]$actual)
pred4_lasso <- prediction(preds_validation_lasso[[4]]$p1, preds_validation_lasso[[4]]$actual)
perf_lasso <- performance(pred_lasso, "tpr", "fpr" )
perf2_lasso <- performance(pred2_lasso, "tpr", "fpr")
perf3_lasso <- performance(pred3_lasso, "tpr", "fpr" )
perf4_lasso <- performance(pred4_lasso, "tpr", "fpr")
plot(perf_lasso, col = "red", main = "ROC for final LASSO model \non validation set for all outcomes") + abline(coef = c(0,1)) 
plot(perf2_lasso, add = TRUE, col = "blue")
plot(perf3_lasso, add = TRUE, col = "green")
plot(perf4_lasso, add = TRUE, col = "orange")

legend("bottomright", 
 legend=c(paste("hv_drink with AUC = ",round(auc_validation_lasso[[2]], 3)),paste("i_drug with AUC = ",round(auc_validation_lasso[[1]], 3)),paste("p_drug with AUC = ",round(auc_validation_lasso[[3]], 3)), paste("prob_drink with AUC = ",round(auc_validation_lasso[[4]], 3))),
 bty = "n",lwd=2, cex=0.8,y.intersp=1.0, col=c("blue","red","green", "orange"), lty=c(1,1,1,1))
```


legend("bottomright", 
 legend=c(paste("hv_drink with AUC = ",round(auc_testing_rf[[2]], 3)),paste("i_drug with AUC = ",round(auc_testing_rf[[1]], 3)),paste("p_drug with AUC = ",round(auc_testing_rf[[3]], 3)), paste("prob_drink with AUC = ",round(auc_testing_rf[[4]], 3))),
##### ROC graph for final RF model on testing set for all outcomes
```{r}
#testing set and rf
pred2_testing <- prediction(preds_testing_rf[[1]]$p1, preds_testing_rf[[1]]$actual)
pred3_testing <- prediction(preds_testing_rf[[2]]$p1, preds_testing_rf[[2]]$actual)
pred4_testing <- prediction(preds_testing_rf[[3]]$p1, preds_testing_rf[[3]]$actual)
#pred4_testing <- prediction(preds_testing_rf[[4]]$p1, preds_testing_rf[[4]]$actual)
perf2_testing <- performance(pred2_testing, "tpr", "fpr" )
perf3_testing <- performance(pred3_testing, "tpr", "fpr")
perf4_testing <- performance(pred4_testing, "tpr", "fpr" )
#perf4_testing <- performance(pred4_testing, "tpr", "fpr")
plot(perf2_testing, col = "red", main = "ROC for final RF model \non testing set for all outcomes") + abline(coef = c(0,1)) 
plot(perf3_testing, add = TRUE, col = "blue")
plot(perf4_testing, add = TRUE, col = "green")
plot(perf_testing, add = TRUE, col = "yellow")

legend("bottomright", 
 legend=c(paste("Suic. idea. with AUC = ",round(auc_testing_rf[[2]], 3)),paste("Ill. drug with AUC = ",round(auc_testing_rf[[1]], 3)),paste("Prob. drink with AUC = ",round(auc_testing_rf[[3]], 3)), paste("Rx abuse with AUC = 0.835")),
 bty = "n",lwd=2, cex=0.8,y.intersp=1.0, col=c("blue","red","green", "yellow"), lty=c(1,1,1,1))
```




##### ROC graph for final LASSO model on testing set for all outcomes
```{r}
# testing set and lasso
pred_testing_lasso <- prediction(preds_testing_lasso[[1]]$p1, preds_testing_lasso[[1]]$actual)
pred2_testing_lasso <- prediction(preds_testing_lasso[[2]]$p1, preds_testing_lasso[[2]]$actual)
pred3_testing_lasso <- prediction(preds_testing_lasso[[3]]$p1, preds_testing_lasso[[3]]$actual)
pred4_testing_lasso <- prediction(preds_testing_lasso[[4]]$p1, preds_testing_lasso[[4]]$actual)
perf_testing_lasso <- performance(pred_testing_lasso, "tpr", "fpr" )
perf2_testing_lasso <- performance(pred2_testing_lasso, "tpr", "fpr")
perf3_testing_lasso <- performance(pred3_testing_lasso, "tpr", "fpr" )
perf4_testing_lasso <- performance(pred4_testing_lasso, "tpr", "fpr")
plot(perf_testing_lasso, col = "red", main = "ROC for final LASSO model \non testing set for all outcomes") + abline(coef = c(0,1)) 
plot(perf2_testing_lasso, add = TRUE, col = "blue")
plot(perf3_testing_lasso, add = TRUE, col = "green")
plot(perf4_testing_lasso, add = TRUE, col = "orange")

legend("bottomright", 
 legend=c(paste("hv_drink with AUC = ",round(auc_testing_lasso[[2]], 3)),paste("i_drug with AUC = ",round(auc_testing_lasso[[1]], 3)),paste("p_drug with AUC = ",round(auc_testing_lasso[[3]], 3)), paste("prob_drink with AUC = ",round(auc_testing_lasso[[4]], 3))),
 bty = "n",lwd=2, cex=0.8,y.intersp=1.0, col=c("blue","red","green", "yellow"), lty=c(1,1,1,1))
```


